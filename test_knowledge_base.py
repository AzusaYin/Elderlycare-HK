#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•å·¥å…·
æ”¯æŒPDFå’ŒMarkdownæ–‡æ¡£çš„ä¸Šä¼ ã€ç´¢å¼•æ„å»ºå’Œæ£€ç´¢æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    python test_knowledge_base.py --help
    python test_knowledge_base.py upload sample.pdf
    python test_knowledge_base.py query "é¦™æ¸¯å®‰è€æœåŠ¡æœ‰å“ªäº›ï¼Ÿ"
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# PDFå¤„ç†
try:
    import fitz  # PyMuPDF
    HAS_PDF = True
except ImportError:
    HAS_PDF = False
    print("âš ï¸  è­¦å‘Š: PyMuPDFæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†PDFæ–‡ä»¶")
    print("   å®‰è£…å‘½ä»¤: pip install PyMuPDF")

# RAGä¾èµ–
try:
    import numpy as np
    import faiss
    from rank_bm25 import BM25Okapi
    from sentence_transformers import SentenceTransformer
    HAS_RAG = True
except ImportError as e:
    HAS_RAG = False
    print(f"âš ï¸  è­¦å‘Š: RAGä¾èµ–æœªå®‰è£…: {e}")
    print("   å®‰è£…å‘½ä»¤: pip install faiss-cpu rank-bm25 sentence-transformers numpy")


# ============ é…ç½® ============
TEST_DOCS_DIR = Path("test_data/docs")
TEST_INDEX_DIR = Path("test_data/index")
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200
TOP_K = 5


# ============ PDFå¤„ç† ============
def extract_text_from_pdf(pdf_path: Path) -> str:
    """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œä¿ç•™é¡µç æ ‡è®°"""
    if not HAS_PDF:
        raise ImportError("PyMuPDFæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†PDFæ–‡ä»¶")

    print(f"ğŸ“„ æ­£åœ¨æå–PDF: {pdf_path.name}")
    doc = fitz.open(str(pdf_path))
    pages_text = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        pages_text.append(f"Page {page_num + 1}\n{text}")
        print(f"   âœ“ å·²æå–ç¬¬ {page_num + 1}/{len(doc)} é¡µ")

    doc.close()
    full_text = "\n\n".join(pages_text)
    print(f"âœ… PDFæå–å®Œæˆ: å…± {len(doc)} é¡µ, {len(full_text)} å­—ç¬¦\n")
    return full_text


# ============ æ–‡æ¡£è¯»å– ============
def read_documents(docs_dir: Path) -> List[Dict]:
    """è¯»å–ç›®å½•ä¸­çš„æ‰€æœ‰Markdownå’ŒPDFæ–‡ä»¶"""
    if not docs_dir.exists():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
        return []

    # æ”¶é›†æ–‡ä»¶
    md_files = list(docs_dir.glob("**/*.md")) + list(docs_dir.glob("**/*.markdown"))
    pdf_files = list(docs_dir.glob("**/*.pdf")) if HAS_PDF else []

    docs = []

    # å¤„ç†Markdown
    for p in md_files:
        try:
            text = p.read_text(encoding="utf-8", errors="ignore")
            docs.append({"path": str(p), "filename": p.name, "text": text, "type": "markdown"})
            print(f"ğŸ“ è¯»å–Markdown: {p.name} ({len(text)} å­—ç¬¦)")
        except Exception as e:
            print(f"âš ï¸  è¯»å–å¤±è´¥ {p.name}: {e}")

    # å¤„ç†PDF
    for p in pdf_files:
        try:
            text = extract_text_from_pdf(p)
            docs.append({"path": str(p), "filename": p.name, "text": text, "type": "pdf"})
        except Exception as e:
            print(f"âš ï¸  è¯»å–å¤±è´¥ {p.name}: {e}")

    return docs


# ============ æ–‡æœ¬åˆ†å— ============
def simple_chunk(text: str, chunk_size: int, overlap: int) -> List[str]:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†å—"""
    step = max(1, chunk_size - overlap)
    n = len(text)
    if n == 0:
        return []
    return [text[i:i+chunk_size] for i in range(0, n, step)]


_SENT_SPLIT = re.compile(r"[ã€‚ï¼ï¼Ÿï¼›ï¼š]\s*")

def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """ä¸­æ–‡å‹å¥½çš„å¥å­æ„ŸçŸ¥åˆ†å—"""
    parts = []
    segs = [s for s in _SENT_SPLIT.split(text) if s]
    for seg in segs:
        parts.extend(simple_chunk(seg, chunk_size, overlap))
    if not segs:
        parts = simple_chunk(text, chunk_size, overlap)
    return parts


# ============ é¡µç æ¨æ–­ ============
_PAGE_PATTERNS = [
    r"(?:^|\n)\s*Page\s*(\d+)\s*(?:\n|$)",
    r"(?:^|\n)\s*p\.\s*(\d+)\s*(?:\n|$)",
    r"(?:^|\n)\s*é \s*(\d+)\s*(?:\n|$)",
    r"(?:^|\n)\s*ç¬¬\s*(\d+)\s*é \s*(?:\n|$)",
]
_PAGE_RE = re.compile("|".join(f"(?:{p})" for p in _PAGE_PATTERNS), re.IGNORECASE)

def infer_pages(text: str) -> List[Dict]:
    """æ¨æ–­æ–‡æœ¬ä¸­çš„é¡µç ä¿¡æ¯"""
    matches = list(_PAGE_RE.finditer(text))
    if not matches:
        return [{"page": None, "start": 0, "end": len(text)}]

    def first_int(m):
        for g in m.groups():
            if g and g.isdigit():
                return int(g)
        return None

    pages = []
    for i, m in enumerate(matches):
        page_no = first_int(m)
        if page_no is None:
            continue
        start = m.start()
        end = matches[i+1].start() if i+1 < len(matches) else len(text)
        pages.append({"page": page_no, "start": start, "end": end})
    return pages


# ============ ç´¢å¼•æ„å»º ============
class KnowledgeBase:
    def __init__(self, model_name: str = EMBEDDING_MODEL):
        if not HAS_RAG:
            raise ImportError("RAGä¾èµ–æœªå®‰è£…")

        print(f"ğŸ¤– åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.chunks = []
        self.bm25 = None
        self.bm25_tokens = []

    def build_index(self, docs: List[Dict]):
        """æ„å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        print(f"\nğŸ“š å¼€å§‹æ„å»ºç´¢å¼• (å…± {len(docs)} ä¸ªæ–‡æ¡£)...")

        all_chunks = []
        for doc in docs:
            pages = infer_pages(doc["text"])
            print(f"   ğŸ“– {doc['filename']}: {len(pages)} é¡µ")

            for page_info in pages:
                page_text = doc["text"][page_info["start"]:page_info["end"]]
                pieces = chunk_text(page_text, CHUNK_SIZE, CHUNK_OVERLAP)

                for i, piece in enumerate(pieces):
                    all_chunks.append({
                        "text": piece,
                        "file": doc["filename"],
                        "page": page_info["page"],
                        "chunk_id": i
                    })

        print(f"   âœ‚ï¸  åˆ†å—å®Œæˆ: {len(all_chunks)} ä¸ªç‰‡æ®µ")

        if not all_chunks:
            print("âŒ æ²¡æœ‰å¯ç´¢å¼•çš„å†…å®¹")
            return

        self.chunks = all_chunks
        texts = [c["text"] for c in all_chunks]

        # ç”ŸæˆåµŒå…¥
        print("   ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡...")
        embeddings = np.array(self.model.encode(texts, normalize_embeddings=True))

        # æ„å»ºFAISSç´¢å¼•
        print("   ğŸ” æ„å»ºFAISSç´¢å¼•...")
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dim)
        self.index.add(embeddings.astype(np.float32))

        # æ„å»ºBM25ç´¢å¼•
        print("   ğŸ“Š æ„å»ºBM25ç´¢å¼•...")
        self.bm25_tokens = [self._tokenize(t) for t in texts]
        self.bm25 = BM25Okapi(self.bm25_tokens)

        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ!\n")

    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•çš„åˆ†è¯ï¼ˆè‹±æ–‡è¯ + ä¸­æ–‡2-gramï¼‰"""
        tokens = re.findall(r"[A-Za-z0-9_]+", text.lower())

        # ä¸­æ–‡2-gram
        cjk = re.compile(r"[\u4e00-\u9fff]")
        if cjk.search(text):
            cjk_chars = "".join(ch for ch in text if cjk.match(ch))
            if len(cjk_chars) >= 2:
                tokens += [cjk_chars[i:i+2] for i in range(len(cjk_chars)-1)]

        return tokens

    def search(self, query: str, k: int = TOP_K) -> List[Dict]:
        """æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25"""
        if self.index is None:
            print("âŒ ç´¢å¼•æœªæ„å»º")
            return []

        print(f"\nğŸ” æœç´¢: {query}")

        # å‘é‡æ£€ç´¢
        q_emb = np.array(self.model.encode([query], normalize_embeddings=True))
        D, I = self.index.search(q_emb.astype(np.float32), min(k * 2, len(self.chunks)))

        vec_hits = {int(I[0][i]): float(D[0][i]) for i in range(len(I[0]))}

        # BM25æ£€ç´¢
        q_tokens = self._tokenize(query)
        bm25_scores = self.bm25.get_scores(q_tokens) if self.bm25 else []
        bm25_hits = {i: float(s) for i, s in enumerate(bm25_scores)} if bm25_scores.size > 0 else {}

        # èåˆåˆ†æ•°
        is_cjk = bool(re.search(r"[\u4e00-\u9fff]", query))
        alpha_vec = 0.60 if is_cjk else 0.90
        alpha_bm25 = 0.40 if is_cjk else 0.10

        combined = {}
        all_ids = set(vec_hits.keys()) | set(bm25_hits.keys())

        for idx in all_ids:
            vec_score = vec_hits.get(idx, 0.0)
            bm25_score = bm25_hits.get(idx, 0.0)
            combined[idx] = alpha_vec * vec_score + alpha_bm25 * bm25_score

        # æ’åºå¹¶è¿”å›
        sorted_ids = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k]

        results = []
        for idx, score in sorted_ids:
            chunk = self.chunks[idx]
            results.append({
                "score": score,
                "text": chunk["text"],
                "file": chunk["file"],
                "page": chunk["page"],
                "chunk_id": chunk["chunk_id"]
            })

        return results

    def save(self, index_dir: Path):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        index_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, str(index_dir / "faiss.index"))

        # ä¿å­˜å…ƒæ•°æ®
        with open(index_dir / "meta.json", "w", encoding="utf-8") as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)

        # ä¿å­˜BM25
        with open(index_dir / "bm25.json", "w", encoding="utf-8") as f:
            json.dump({"tokens": self.bm25_tokens}, f, ensure_ascii=False)

        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ°: {index_dir}")

    def load(self, index_dir: Path):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        if not index_dir.exists():
            print(f"âŒ ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {index_dir}")
            return False

        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(str(index_dir / "faiss.index"))

        # åŠ è½½å…ƒæ•°æ®
        with open(index_dir / "meta.json", "r", encoding="utf-8") as f:
            self.chunks = json.load(f)

        # åŠ è½½BM25
        with open(index_dir / "bm25.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            self.bm25_tokens = data["tokens"]
            self.bm25 = BM25Okapi(self.bm25_tokens)

        print(f"ğŸ“‚ ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªç‰‡æ®µ")
        return True


# ============ å‘½ä»¤è¡Œæ¥å£ ============
def cmd_upload(args):
    """ä¸Šä¼ æ–‡æ¡£åˆ°æµ‹è¯•çŸ¥è¯†åº“"""
    file_path = Path(args.file)
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    # åˆ›å»ºæµ‹è¯•ç›®å½•
    TEST_DOCS_DIR.mkdir(parents=True, exist_ok=True)

    # å¤åˆ¶æ–‡ä»¶
    import shutil
    dest = TEST_DOCS_DIR / file_path.name
    shutil.copy(file_path, dest)
    print(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {dest}\n")

    # é‡å»ºç´¢å¼•
    if args.rebuild:
        cmd_build(args)


def cmd_build(args):
    """æ„å»ºæˆ–é‡å»ºç´¢å¼•"""
    docs = read_documents(TEST_DOCS_DIR)
    if not docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return

    kb = KnowledgeBase()
    kb.build_index(docs)
    kb.save(TEST_INDEX_DIR)


def cmd_query(args):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""
    kb = KnowledgeBase()

    # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
    if not kb.load(TEST_INDEX_DIR):
        print("\nâŒ ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•")
        print("   å‘½ä»¤: python test_knowledge_base.py upload <your_file.pdf>")
        return

    # æ‰§è¡ŒæŸ¥è¯¢
    results = kb.search(args.query, k=args.top_k)

    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ:\n")
    print("=" * 80)

    for i, r in enumerate(results, 1):
        print(f"\nã€ç»“æœ {i}ã€‘ (å¾—åˆ†: {r['score']:.4f})")
        print(f"æ–‡ä»¶: {r['file']}")
        if r['page']:
            print(f"é¡µç : {r['page']}")
        print(f"\nå†…å®¹é¢„è§ˆ:\n{r['text'][:300]}...")
        print("-" * 80)


def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
    docs = read_documents(TEST_DOCS_DIR)

    if not docs:
        print("ğŸ“‚ æ–‡æ¡£ç›®å½•ä¸ºç©º")
        return

    print(f"\nğŸ“š å…± {len(docs)} ä¸ªæ–‡æ¡£:\n")
    for doc in docs:
        size_kb = len(doc['text']) / 1024
        print(f"  â€¢ {doc['filename']} ({doc['type']}) - {size_kb:.1f} KB")


def cmd_demo(args):
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    print("=" * 80)
    print("ğŸ¯ çŸ¥è¯†åº“ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 80)

    # æ£€æŸ¥ç¤ºä¾‹æ–‡ä»¶
    if not TEST_DOCS_DIR.exists() or not list(TEST_DOCS_DIR.iterdir()):
        print("\nğŸ“ åˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
        TEST_DOCS_DIR.mkdir(parents=True, exist_ok=True)

        sample_text = """Page 1
é¦™æ¸¯å®‰è€æœåŠ¡æ¦‚è¿°

é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºä¸ºé•¿è€…æä¾›å¤šå…ƒåŒ–çš„å®‰è€æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š

1. ç¤¾åŒºç…§é¡¾æœåŠ¡
   - é•¿è€…ä¸­å¿ƒ
   - æ—¥é—´æŠ¤ç†ä¸­å¿ƒ
   - å®¶å±…ç…§é¡¾æœåŠ¡

2. é™¢èˆç…§é¡¾æœåŠ¡
   - å®‰è€é™¢
   - æŠ¤ç†å®‰è€é™¢
   - æŠ¤å…»é™¢

Page 2
ç”³è¯·èµ„æ ¼

ç”³è¯·å®‰è€æœåŠ¡çš„åŸºæœ¬èµ„æ ¼ï¼š
- å¹´é¾„ï¼š60å²æˆ–ä»¥ä¸Š
- å±…ä½ï¼šé¦™æ¸¯æ°¸ä¹…æ€§å±…æ°‘
- éœ€è¦ï¼šç»ç¤¾å·¥è¯„ä¼°ç¡®è®¤éœ€è¦æœåŠ¡

æŸ¥è¯¢çƒ­çº¿ï¼š2343 2255
ç½‘å€ï¼šwww.swd.gov.hk
"""

        (TEST_DOCS_DIR / "sample.md").write_text(sample_text, encoding="utf-8")
        print("   âœ“ å·²åˆ›å»ºç¤ºä¾‹æ–‡æ¡£: sample.md")

    # æ„å»ºç´¢å¼•
    print("\n" + "=" * 80)
    class Args:
        rebuild = True
    cmd_build(Args())

    # ç¤ºä¾‹æŸ¥è¯¢
    print("\n" + "=" * 80)
    print("ğŸ” ç¤ºä¾‹æŸ¥è¯¢")
    print("=" * 80)

    queries = [
        "é¦™æ¸¯å®‰è€æœåŠ¡æœ‰å“ªäº›ï¼Ÿ",
        "ç”³è¯·èµ„æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æŸ¥è¯¢çƒ­çº¿"
    ]

    for q in queries:
        class QueryArgs:
            query = q
            top_k = 2
        cmd_query(QueryArgs())
        print("\n")


def main():
    parser = argparse.ArgumentParser(
        description="çŸ¥è¯†åº“ç³»ç»Ÿæµ‹è¯•å·¥å…· - æ”¯æŒPDFå’ŒMarkdown",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸Šä¼ PDFæ–‡ä»¶å¹¶è‡ªåŠ¨æ„å»ºç´¢å¼•
  python test_knowledge_base.py upload document.pdf

  # ä¸Šä¼ Markdownæ–‡ä»¶
  python test_knowledge_base.py upload notes.md

  # æŸ¥è¯¢çŸ¥è¯†åº“
  python test_knowledge_base.py query "é¦™æ¸¯å®‰è€æœåŠ¡æœ‰å“ªäº›ï¼Ÿ"

  # åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
  python test_knowledge_base.py list

  # è¿è¡Œå®Œæ•´æ¼”ç¤º
  python test_knowledge_base.py demo
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")

    # uploadå‘½ä»¤
    upload_parser = subparsers.add_parser("upload", help="ä¸Šä¼ æ–‡æ¡£")
    upload_parser.add_argument("file", help="æ–‡ä»¶è·¯å¾„ (.pdf æˆ– .md)")
    upload_parser.add_argument("--no-rebuild", dest="rebuild", action="store_false",
                              help="ä¸è‡ªåŠ¨é‡å»ºç´¢å¼•")

    # buildå‘½ä»¤
    build_parser = subparsers.add_parser("build", help="æ„å»ºç´¢å¼•")

    # queryå‘½ä»¤
    query_parser = subparsers.add_parser("query", help="æŸ¥è¯¢çŸ¥è¯†åº“")
    query_parser.add_argument("query", help="æŸ¥è¯¢å†…å®¹")
    query_parser.add_argument("-k", "--top-k", type=int, default=TOP_K,
                             help=f"è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: {TOP_K})")

    # listå‘½ä»¤
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£")

    # demoå‘½ä»¤
    demo_parser = subparsers.add_parser("demo", help="è¿è¡Œæ¼”ç¤º")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # æ‰§è¡Œå‘½ä»¤
    commands = {
        "upload": cmd_upload,
        "build": cmd_build,
        "query": cmd_query,
        "list": cmd_list,
        "demo": cmd_demo
    }

    commands[args.command](args)


if __name__ == "__main__":
    main()
