import json
import threading
import faiss
import numpy as np
import re
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer

from .security import encrypt_bytes, decrypt_bytes
from .utils import read_markdown_files, read_all_documents, infer_page_map

DOCS_DIR = Path("data/docs")

# PDFé¡µç æ ‡è®°æ­£åˆ™ï¼ˆç”¨äºä»PDFæå–çš„æ–‡æœ¬ä¸­è¯†åˆ«é¡µç ï¼‰
_PDF_PAGE_MARKER = re.compile(r"---\s*Page\s*(\d+)\s*---")

@dataclass
class Chunk:
    text: str
    meta: Dict

class Embedder:
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model = SentenceTransformer(model_name, device=device)
    def encode(self, texts: List[str]) -> np.ndarray:
        return np.array(self.model.encode(texts, normalize_embeddings=True))

class Index:
    def __init__(self, index_dir: str):
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        self.faiss = None
        self.meta: List[Dict] = []
        self.bm25 = None
        self.bm25_corpus_tokens: List[List[str]] = []
        # æ–‡ä»¶å“ˆå¸Œè®°å½•ï¼ˆç”¨äºå¢é‡ç´¢å¼•ï¼‰
        self.file_hashes: Dict[str, str] = {}

    def _compute_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        import hashlib
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""

    def save(self):
        # faiss index
        if self.faiss is not None:
            raw_path = self.index_dir / "faiss.index"
            faiss.write_index(self.faiss, str(raw_path))
            # ç”¨åŠ å¯†è¦†ç›–å†™å›ï¼›encrypt_bytes å†…éƒ¨ä¼šæ ¹æ® settings.encrypt_data å†³å®šæ˜¯å¦åŠ å¯†/æŠ¥é”™
            data = raw_path.read_bytes()
            raw_path.write_bytes(encrypt_bytes(data))

        # meta.json
        meta_path = self.index_dir / "meta.json"
        meta_bytes = json.dumps(self.meta, ensure_ascii=False).encode("utf-8")
        meta_path.write_bytes(encrypt_bytes(meta_bytes))

        # bm25.json
        if self.bm25 is not None:
            bm25_path = self.index_dir / "bm25.json"
            bm = {"tokens": self.bm25_corpus_tokens}
            bm_bytes = json.dumps(bm, ensure_ascii=False).encode("utf-8")
            bm25_path.write_bytes(encrypt_bytes(bm_bytes))

        # file_hashes.jsonï¼ˆç”¨äºå¢é‡ç´¢å¼•ï¼‰
        hashes_path = self.index_dir / "file_hashes.json"
        hashes_bytes = json.dumps(self.file_hashes, ensure_ascii=False).encode("utf-8")
        hashes_path.write_bytes(encrypt_bytes(hashes_bytes))

    def load(self):
        # faiss
        faiss_path = self.index_dir / "faiss.index"
        meta_path = self.index_dir / "meta.json"
        if faiss_path.exists() and meta_path.exists():
            blob = decrypt_bytes(faiss_path.read_bytes())
            tmp = self.index_dir / ".faiss.tmp"
            tmp.write_bytes(blob)
            self.faiss = faiss.read_index(str(tmp))
            tmp.unlink(missing_ok=True)

            mb = decrypt_bytes(meta_path.read_bytes())
            self.meta = json.loads(mb.decode("utf-8"))

        # bm25
        bm25_path = self.index_dir / "bm25.json"
        if bm25_path.exists():
            bb = decrypt_bytes(bm25_path.read_bytes())
            data = json.loads(bb.decode("utf-8"))
            self.bm25_corpus_tokens = data.get("tokens", [])
            if self.bm25_corpus_tokens:
                self.bm25 = BM25Okapi(self.bm25_corpus_tokens)

        # file_hashes.json
        hashes_path = self.index_dir / "file_hashes.json"
        if hashes_path.exists():
            try:
                hb = decrypt_bytes(hashes_path.read_bytes())
                self.file_hashes = json.loads(hb.decode("utf-8"))
            except Exception:
                self.file_hashes = {}

    def build(self, embeddings: np.ndarray, meta: List[Dict], bm25_tokens: Optional[List[List[str]]] = None):
        dim = embeddings.shape[1] if embeddings.size else 384
        index = faiss.IndexFlatIP(dim)
        if embeddings.size:
            index.add(embeddings.astype(np.float32))
        self.faiss = index
        self.meta = meta
        if bm25_tokens:
            self.bm25_corpus_tokens = bm25_tokens
            self.bm25 = BM25Okapi(self.bm25_corpus_tokens)

    def add_vectors(self, embeddings: np.ndarray, meta: List[Dict], bm25_tokens: Optional[List[List[str]]] = None):
        """
        å¢é‡æ·»åŠ å‘é‡åˆ°ç°æœ‰ç´¢å¼•ã€‚
        ç”¨äºå¢é‡ç´¢å¼•åœºæ™¯ã€‚
        """
        if self.faiss is None:
            # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œç›´æ¥build
            self.build(embeddings, meta, bm25_tokens)
            return

        if embeddings.size == 0:
            return

        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.faiss.add(embeddings.astype(np.float32))

        # æ·»åŠ å…ƒæ•°æ®
        self.meta.extend(meta)

        # æ›´æ–°BM25ï¼ˆéœ€è¦é‡å»ºï¼Œå› ä¸ºBM25ä¸æ”¯æŒå¢é‡ï¼‰
        if bm25_tokens:
            self.bm25_corpus_tokens.extend(bm25_tokens)
            self.bm25 = BM25Okapi(self.bm25_corpus_tokens)

    def remove_file(self, file_path: str):
        """
        ä»ç´¢å¼•ä¸­ç§»é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰chunkã€‚
        æ³¨æ„ï¼šFAISS IndexFlatIPä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œéœ€è¦é‡å»ºã€‚
        """
        if not self.meta:
            return

        # æ‰¾å‡ºè¦ä¿ç•™çš„ç´¢å¼•
        keep_indices = []
        for i, m in enumerate(self.meta):
            if m.get("file") != file_path:
                keep_indices.append(i)

        if len(keep_indices) == len(self.meta):
            # æ²¡æœ‰éœ€è¦åˆ é™¤çš„
            return

        # é‡å»ºç´¢å¼•ï¼ˆåªä¿ç•™éœ€è¦çš„éƒ¨åˆ†ï¼‰
        # è¿™æ˜¯ä¸€ä¸ªæ˜‚è´µçš„æ“ä½œï¼Œä½†FAISS IndexFlatIPä¸æ”¯æŒåˆ é™¤
        if keep_indices:
            # è·å–éœ€è¦ä¿ç•™çš„å‘é‡
            all_vectors = faiss.rev_swig_ptr(self.faiss.get_xb(), self.faiss.ntotal * self.faiss.d)
            all_vectors = all_vectors.reshape(self.faiss.ntotal, self.faiss.d)
            kept_vectors = all_vectors[keep_indices]

            # é‡å»ºFAISSç´¢å¼•
            dim = self.faiss.d
            new_index = faiss.IndexFlatIP(dim)
            new_index.add(kept_vectors.astype(np.float32))
            self.faiss = new_index

            # æ›´æ–°å…ƒæ•°æ®
            self.meta = [self.meta[i] for i in keep_indices]

            # æ›´æ–°BM25
            if self.bm25_corpus_tokens:
                self.bm25_corpus_tokens = [self.bm25_corpus_tokens[i] for i in keep_indices]
                self.bm25 = BM25Okapi(self.bm25_corpus_tokens) if self.bm25_corpus_tokens else None
        else:
            # å…¨éƒ¨åˆ é™¤ï¼Œæ¸…ç©ºç´¢å¼•
            dim = self.faiss.d if self.faiss else 384
            self.faiss = faiss.IndexFlatIP(dim)
            self.meta = []
            self.bm25_corpus_tokens = []
            self.bm25 = None

        # æ›´æ–°æ–‡ä»¶å“ˆå¸Œ
        if file_path in self.file_hashes:
            del self.file_hashes[file_path]

    def get_indexed_files(self) -> set:
        """è·å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
        return set(m.get("file") for m in self.meta if m.get("file"))

    def search(self, query_emb: np.ndarray, k: int) -> List[Tuple[int, float]]:
        if self.faiss is None or self.faiss.ntotal == 0:
            return []
        D, I = self.faiss.search(query_emb.astype(np.float32), k)
        return list(zip(I[0].tolist(), D[0].tolist()))

# --- Chunking (safe & fast) ---
def simple_char_chunk(text: str, chunk_size: int, overlap: int) -> List[str]:
    step = max(1, chunk_size - overlap)
    n = len(text)
    if n == 0:
        return []
    return [text[i:i+chunk_size] for i in range(0, n, step)]

# --- Sentence-aware chunking for CJK ---
_SENT_SPLIT = re.compile(r"[ã€‚ï¼ï¼Ÿï¼›ï¼š]\s*")  # ç²—ç²’åº¦åˆ†å¥

# --- Enhanced semantic chunking ---
# æ®µè½åˆ†éš”ç¬¦
_PARA_SPLIT = re.compile(r'\n\s*\n')
# å¥å­åˆ†éš”ç¬¦ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
_SENTENCE_SPLIT = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+|(?<=\n)')
# æ ‡é¢˜æ£€æµ‹ï¼ˆMarkdowné£æ ¼ï¼‰
_HEADING_RE = re.compile(r'^#{1,6}\s+.+$|^.+\n[=\-]{2,}$', re.MULTILINE)

def semantic_chunk(text: str, chunk_size: int, overlap: int, respect_paragraphs: bool = True) -> List[str]:
    """
    è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬åˆ†å—ï¼š
    1. é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
    2. å¦‚æœæ®µè½å¤ªé•¿ï¼ŒæŒ‰å¥å­åˆ†å‰²
    3. åˆå¹¶å°æ®µè½ç›´åˆ°è¾¾åˆ°chunk_size
    4. ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    """
    if not text or not text.strip():
        return []

    chunks = []
    current_chunk = ""

    # æ­¥éª¤1: æŒ‰æ®µè½åˆ†å‰²
    if respect_paragraphs:
        paragraphs = _PARA_SPLIT.split(text)
    else:
        paragraphs = [text]

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # å¦‚æœæ®µè½æœ¬èº«å°±å¾ˆçŸ­ï¼Œå°è¯•åˆå¹¶
        if len(current_chunk) + len(para) + 2 <= chunk_size:
            if current_chunk:
                current_chunk += "\n\n" + para
            else:
                current_chunk = para
            continue

        # å¦‚æœå½“å‰chunkå·²ç»æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜
        if current_chunk:
            chunks.append(current_chunk)
            # ä¿ç•™overlapéƒ¨åˆ†
            if overlap > 0 and len(current_chunk) > overlap:
                current_chunk = current_chunk[-overlap:]
            else:
                current_chunk = ""

        # å¦‚æœæ®µè½å¤ªé•¿ï¼ŒæŒ‰å¥å­åˆ†å‰²
        if len(para) > chunk_size:
            sentences = _split_into_sentences(para)
            for sent in sentences:
                sent = sent.strip()
                if not sent:
                    continue

                if len(current_chunk) + len(sent) + 1 <= chunk_size:
                    if current_chunk:
                        current_chunk += " " + sent
                    else:
                        current_chunk = sent
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡chunk_sizeï¼Œå¼ºåˆ¶åˆ†å‰²
                    if len(sent) > chunk_size:
                        sub_chunks = simple_char_chunk(sent, chunk_size, overlap)
                        chunks.extend(sub_chunks[:-1])
                        current_chunk = sub_chunks[-1] if sub_chunks else ""
                    else:
                        current_chunk = sent
        else:
            current_chunk = para

    # ä¿å­˜æœ€åä¸€ä¸ªchunk
    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def _split_into_sentences(text: str) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    # ä¸­æ–‡å¥å­ç»“æŸæ ‡å¿—
    cn_pattern = r'([ã€‚ï¼ï¼Ÿï¼›])'
    # è‹±æ–‡å¥å­ç»“æŸæ ‡å¿—
    en_pattern = r'([.!?])\s+'

    # å…ˆæŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²
    parts = re.split(cn_pattern, text)
    sentences = []
    i = 0
    while i < len(parts):
        sent = parts[i]
        # å¦‚æœä¸‹ä¸€ä¸ªæ˜¯æ ‡ç‚¹ï¼Œåˆå¹¶
        if i + 1 < len(parts) and re.match(cn_pattern, parts[i+1]):
            sent += parts[i+1]
            i += 2
        else:
            i += 1
        if sent.strip():
            sentences.append(sent.strip())

    # å†å¯¹æ¯ä¸ªéƒ¨åˆ†æŒ‰è‹±æ–‡æ ‡ç‚¹åˆ†å‰²
    final_sentences = []
    for sent in sentences:
        if re.search(r'[.!?]\s+[A-Z]', sent):
            # åŒ…å«è‹±æ–‡å¥å­
            en_parts = re.split(en_pattern, sent)
            j = 0
            while j < len(en_parts):
                s = en_parts[j]
                if j + 1 < len(en_parts) and re.match(en_pattern, en_parts[j+1] + ' '):
                    s += en_parts[j+1]
                    j += 2
                else:
                    j += 1
                if s.strip():
                    final_sentences.append(s.strip())
        else:
            final_sentences.append(sent)

    return final_sentences

def chunk_text(text: str, chunk_size: int, overlap: int) -> list[str]:
    """
    æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼š
    - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼ˆä¿æŒæ®µè½å’Œå¥å­å®Œæ•´æ€§ï¼‰
    - å¯¹äºçº¯ä¸­æ–‡æ–‡æœ¬ï¼Œä½¿ç”¨åŸæœ‰çš„CJKæ„ŸçŸ¥åˆ†å—ä½œä¸ºå¤‡é€‰
    """
    # æ£€æµ‹æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡
    cjk_ratio = len(re.findall(r'[\u4e00-\u9fff]', text)) / max(1, len(text))

    # å°è¯•è¯­ä¹‰åˆ†å—
    chunks = semantic_chunk(text, chunk_size, overlap)

    # å¦‚æœè¯­ä¹‰åˆ†å—æ•ˆæœä¸å¥½ï¼ˆå—æ•°å¤ªå°‘æˆ–å—å¤ªå¤§ï¼‰ï¼Œå›é€€åˆ°åŸæœ‰ç­–ç•¥
    if not chunks:
        # å›é€€åˆ°åŸæœ‰çš„CJKæ„ŸçŸ¥åˆ†å—
        parts = []
        segs = [s for s in _SENT_SPLIT.split(text) if s]
        for seg in segs:
            parts.extend(simple_char_chunk(seg, chunk_size, overlap))
        if not segs:
            parts = simple_char_chunk(text, chunk_size, overlap)
        return parts

    return chunks

# --- Tokenization (CJK-friendly) ---

_CJK = re.compile(r"[\u4e00-\u9fff]")

def _to_halfwidth(s: str) -> str:
    # å…¨è§’è½¬åŠè§’ï¼ˆå¸¸è§ä¸­æ–‡æ•°å­—/æ ‡ç‚¹ï¼‰
    out = []
    for ch in s:
        code = ord(ch)
        if code == 0x3000:  # å…¨è§’ç©ºæ ¼
            code = 0x20
        elif 0xFF01 <= code <= 0xFF5E:
            code -= 0xFEE0
        out.append(chr(code))
    return "".join(out)

def tokenize(text: str) -> list[str]:
    text = _to_halfwidth(text)
    tokens: list[str] = []

    # 1) è‹±æ–‡/æ•°å­—è¯ï¼Œå§‹ç»ˆä¿ç•™
    tokens += re.findall(r"[A-Za-z0-9_]+", text.lower())

    # 2) CJK 2/3-gramï¼Œåªå¯¹ CJK æ®µè½è¿½åŠ 
    if _CJK.search(text):
        # åªå– CJK å­—ç¬¦åš n-gramï¼Œé¿å…æŠŠè‹±æ–‡ä¸€èµ·ç¢¾ç¢
        cjk_only = "".join(ch for ch in re.sub(r"\s+", "", text) if _CJK.match(ch))
        if cjk_only:
            if len(cjk_only) >= 2:
                tokens += [cjk_only[i:i+2] for i in range(len(cjk_only)-1)]
            if len(cjk_only) >= 3:
                tokens += [cjk_only[i:i+3] for i in range(len(cjk_only)-2)]
    return tokens


# --- Ingestion ---
def _get_page_ranges_for_doc(doc: Dict) -> List[Dict]:
    """
    æ ¹æ®æ–‡æ¡£æ ¼å¼è·å–é¡µç èŒƒå›´ã€‚
    å¯¹äºPDFæ–‡æ¡£ï¼Œä¼˜å…ˆä½¿ç”¨read_pdf_fileè¿”å›çš„page_rangesã€‚
    å¯¹äºå…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨infer_page_mapä»æ–‡æœ¬ä¸­æ¨æ–­ã€‚
    """
    # å¦‚æœæ–‡æ¡£å·²æœ‰page_rangesï¼ˆæ¥è‡ªPDFè§£æï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    if "page_ranges" in doc and doc["page_ranges"]:
        return doc["page_ranges"]

    # å¦åˆ™ä»æ–‡æœ¬ä¸­æ¨æ–­é¡µç 
    return infer_page_map(doc["text"])

def _clean_text(text: str) -> str:
    """
    æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰
    """
    if not text:
        return ""

    # å»é™¤å¤šä½™ç©ºç™½è¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)

    # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join(lines)

    # å»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦ï¼‰
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

    return text.strip()

def ingest_corpus(docs_dir: str, index_dir: str) -> Tuple[int, int]:
    docs = read_all_documents(docs_dir)
    print(f"[ingest] loaded {len(docs)} doc(s)")
    embedder = Embedder(settings.embedding_model_name, settings.embedding_device)

    all_chunks: List[Chunk] = []
    for di, doc in enumerate(docs, 1):
        doc_format = doc.get("format", "unknown")
        page_ranges = _get_page_ranges_for_doc(doc)
        print(f"[ingest] doc {di}/{len(docs)} ({doc_format}) -> {len(page_ranges)} page(s)")

        for pr in page_ranges:
            page_text = doc["text"][pr["start"]:pr["end"]]
            # æ¸…æ´—æ–‡æœ¬
            page_text = _clean_text(page_text)
            if not page_text.strip():
                continue

            pieces = chunk_text(page_text, settings.chunk_size, settings.chunk_overlap)
            for i, piece in enumerate(pieces):
                if not piece.strip():
                    continue
                meta = {
                    "file": doc["path"],
                    "page": pr["page"],
                    "chunk_id": i,
                    "text": piece,
                    "format": doc_format
                }
                all_chunks.append(Chunk(text=piece, meta=meta))

    print(f"[ingest] total chunks: {len(all_chunks)}; start embedding...")

    if not all_chunks:
        index = Index(index_dir)
        index.build(np.zeros((0, 384), dtype=np.float32), [], None)
        index.save()
        return len(docs), 0

    texts = [c.text for c in all_chunks]

    # batch embedding
    BATCH = 512
    emb_list = []
    for i in range(0, len(texts), BATCH):
        emb_list.append(embedder.encode(texts[i:i+BATCH]))
    embeddings = np.vstack(emb_list).astype(np.float32)
    print(f"[ingest] embedding done, shape={embeddings.shape}")

    bm25_tokens = [tokenize(t) for t in texts] if settings.enable_bm25 else None

    meta = [c.meta for c in all_chunks]
    index = Index(index_dir)
    index.build(embeddings, meta, bm25_tokens)

    # æ›´æ–°æ–‡ä»¶å“ˆå¸Œè®°å½•
    for doc in docs:
        index.file_hashes[doc["path"]] = index._compute_file_hash(doc["path"])

    index.save()
    print("[ingest] index saved")

    return len(docs), len(all_chunks)

def ingest_file_incremental(file_path: str, index_dir: str) -> Tuple[int, bool]:
    """
    å¢é‡ç´¢å¼•å•ä¸ªæ–‡ä»¶ã€‚
    åªå¯¹æ–°æ–‡ä»¶æˆ–å·²ä¿®æ”¹çš„æ–‡ä»¶è¿›è¡Œç´¢å¼•ï¼Œè€Œä¸æ˜¯é‡å»ºæ•´ä¸ªç´¢å¼•ã€‚

    è¿”å›: (chunks_added, was_updated)
    - chunks_added: æ·»åŠ çš„chunkæ•°é‡
    - was_updated: æ–‡ä»¶æ˜¯å¦è¢«æ›´æ–°ï¼ˆTrueï¼‰æˆ–æ˜¯æ–°æ–‡ä»¶ï¼ˆFalseï¼‰
    """
    from .utils import read_document

    # åŠ è½½ç°æœ‰ç´¢å¼•
    index = Index(index_dir)
    index.load()

    embedder = Embedder(settings.embedding_model_name, settings.embedding_device)

    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
    file_path_str = str(file_path)
    new_hash = index._compute_file_hash(file_path_str)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»ç´¢å¼•ä¸”æœªä¿®æ”¹
    old_hash = index.file_hashes.get(file_path_str)
    if old_hash == new_hash and old_hash is not None:
        print(f"[ingest] File {file_path} unchanged, skipping")
        return 0, False

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä½†è¢«ä¿®æ”¹ï¼Œå…ˆåˆ é™¤æ—§çš„
    was_updated = False
    if old_hash is not None:
        print(f"[ingest] File {file_path} modified, removing old chunks")
        index.remove_file(file_path_str)
        was_updated = True

    # è¯»å–æ–‡æ¡£
    doc = read_document(Path(file_path))
    if not doc["text"]:
        print(f"[ingest] File {file_path} is empty or unreadable")
        return 0, was_updated

    # å¤„ç†æ–‡æ¡£
    doc_format = doc.get("format", "unknown")
    page_ranges = _get_page_ranges_for_doc(doc)
    print(f"[ingest] Processing {file_path} ({doc_format}) -> {len(page_ranges)} page(s)")

    all_chunks: List[Chunk] = []
    for pr in page_ranges:
        page_text = doc["text"][pr["start"]:pr["end"]]
        page_text = _clean_text(page_text)
        if not page_text.strip():
            continue

        pieces = chunk_text(page_text, settings.chunk_size, settings.chunk_overlap)
        for i, piece in enumerate(pieces):
            if not piece.strip():
                continue
            meta = {
                "file": doc["path"],
                "page": pr["page"],
                "chunk_id": i,
                "text": piece,
                "format": doc_format
            }
            all_chunks.append(Chunk(text=piece, meta=meta))

    if not all_chunks:
        print(f"[ingest] No chunks generated from {file_path}")
        # æ›´æ–°æ–‡ä»¶å“ˆå¸Œï¼ˆå³ä½¿æ²¡æœ‰å†…å®¹ï¼‰
        index.file_hashes[file_path_str] = new_hash
        index.save()
        return 0, was_updated

    # ç”ŸæˆåµŒå…¥
    texts = [c.text for c in all_chunks]
    BATCH = 512
    emb_list = []
    for i in range(0, len(texts), BATCH):
        emb_list.append(embedder.encode(texts[i:i+BATCH]))
    embeddings = np.vstack(emb_list).astype(np.float32)
    print(f"[ingest] Incremental embedding done, shape={embeddings.shape}")

    # BM25 tokens
    bm25_tokens = [tokenize(t) for t in texts] if settings.enable_bm25 else None

    # æ·»åŠ åˆ°ç´¢å¼•
    meta = [c.meta for c in all_chunks]
    index.add_vectors(embeddings, meta, bm25_tokens)

    # æ›´æ–°æ–‡ä»¶å“ˆå¸Œ
    index.file_hashes[file_path_str] = new_hash

    index.save()
    print(f"[ingest] Incremental index updated: {len(all_chunks)} chunks added")

    return len(all_chunks), was_updated

def remove_file_from_index(file_path: str, index_dir: str) -> bool:
    """
    ä»ç´¢å¼•ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰chunkã€‚

    è¿”å›: True å¦‚æœæ–‡ä»¶è¢«åˆ é™¤ï¼ŒFalse å¦‚æœæ–‡ä»¶ä¸åœ¨ç´¢å¼•ä¸­
    """
    index = Index(index_dir)
    index.load()

    file_path_str = str(file_path)
    if file_path_str not in index.file_hashes:
        print(f"[ingest] File {file_path} not in index")
        return False

    print(f"[ingest] Removing {file_path} from index")
    index.remove_file(file_path_str)
    index.save()
    print(f"[ingest] File removed from index")

    return True

def exact_phrase_fallback(query: str, limit=3):
    q = (query or "").strip()
    if len(q) < 8:
        return []
    pat = re.escape(q)
    hits = []
    for p in _DOCS_DIR.glob("**/*.md"):
        try:
            txt = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        m = re.search(pat, txt, flags=re.IGNORECASE)
        if not m:
            continue
        s = max(0, m.start() - 300); e = min(len(txt), m.end() + 300)
        hits.append({
            "id": f"{p.name}::0",
            "file": p.name,
            "page": None,
            "score": 1.0,
            "text": txt[s:e]
        })
        if len(hits) >= limit:
            break
    return hits

# --- Retrieval ---
from .settings import settings

_PENALTY = None           # type: dict | None
_PENALTY_MTIME = 0.0
_PENALTY_LOCK = threading.Lock()

def _load_penalty() -> dict:
    """
    æ‡’åŠ è½½ + mtime å˜æ›´æ—¶é‡è½½ï¼š
    è¿”å› { "file.md::12": 0.20, ... }ï¼Œé”®ä¸º æ–‡ä»¶å::é¡µç ï¼›å€¼ä¸ºæ‰£åˆ†(>=0)ã€‚
    """
    global _PENALTY, _PENALTY_MTIME
    p = Path("data/feedback/penalty.json")
    try:
        mtime = p.stat().st_mtime
    except FileNotFoundError:
        with _PENALTY_LOCK:
            _PENALTY = {}
            _PENALTY_MTIME = 0.0
        return {}

    with _PENALTY_LOCK:
        if _PENALTY is None or mtime != _PENALTY_MTIME:
            try:
                _PENALTY = json.loads(p.read_text(encoding="utf-8"))
            except Exception:
                _PENALTY = {}
            _PENALTY_MTIME = mtime
        return _PENALTY or {}

def hybrid_retrieve(query: str, index: Index, embedder: Embedder, k: int, *, soft: bool=False) -> List[Dict]:
    def _tokenize_q(q: str) -> list[str]:
        def _to_halfwidth(s: str) -> str:
            out = []
            for ch in s:
                code = ord(ch)
                if code == 0x3000: code = 0x20
                elif 0xFF01 <= code <= 0xFF5E: code -= 0xFEE0
                out.append(chr(code))
            return re.sub(r"\s+", " ", "".join(out)).strip()

        s = _to_halfwidth(q)
        toks: list[str] = []

        # è‹±æ–‡/æ•°å­—è¯ï¼Œå§‹ç»ˆåŠ å…¥
        toks += re.findall(r"[A-Za-z0-9_]+", s.lower())

        # è‹¥å« CJKï¼Œå†è¿½åŠ  CJK n-gramï¼ˆåªå¯¹ CJK å­—ç¬¦åšï¼‰
        if _CJK.search(s):
            cjk_only = "".join(ch for ch in s.replace(" ", "") if _CJK.match(ch))
            if cjk_only:
                if len(cjk_only) >= 2:
                    toks += [cjk_only[i:i+2] for i in range(len(cjk_only)-1)]
                if len(cjk_only) >= 3:
                    toks += [cjk_only[i:i+3] for i in range(len(cjk_only)-2)]
        return toks

    is_cjk = bool(_CJK.search(query))
    q_emb = embedder.encode([query])

    # å‘é‡æ£€ç´¢
    vec_hits: List[Tuple[int, float]] = []
    if index.faiss is not None and index.faiss.ntotal > 0:
        D, I = index.faiss.search(q_emb.astype(np.float32), max(k, 50))
        vec_hits = [(int(I[0][i]), float(D[0][i])) for i in range(len(I[0]))]

    # BM25ï¼ˆä¸­æ–‡åˆ†è¯æ”¹é€ ï¼‰
    bm25_hits: List[Tuple[int, float]] = []
    if index.bm25 is not None and index.bm25_corpus_tokens:
        q_tokens = _tokenize_q(query)
        if q_tokens:
            scores = index.bm25.get_scores(q_tokens)
            top_ids = np.argsort(scores)[::-1][:max(k, 50)]
            bm25_hits = [(int(i), float(scores[i])) for i in top_ids]

    # åˆå¹¶åˆ†æ•°ï¼ˆæƒé‡éš CJK è°ƒæ•´ï¼‰
    # ä¸­æ–‡ï¼šBM25 æ›´é‡è¦ï¼›è‹±æ–‡ï¼šå‘é‡ä¸ºä¸»
    alpha_vec = 0.60 if is_cjk else 0.90
    alpha_bm25 = 0.40 if is_cjk else 0.10

    score_map: Dict[int, Dict[str, float]] = {}
    for idx_i, sim in vec_hits:
        m = score_map.setdefault(idx_i, {"vec": -1e9, "bm25": -1e9})
        m["vec"] = max(m["vec"], sim)
    for idx_i, s in bm25_hits:
        m = score_map.setdefault(idx_i, {"vec": -1e9, "bm25": -1e9})
        m["bm25"] = max(m["bm25"], s)

    # è‡ªé€‚åº”é˜ˆå€¼
    vec_thr = settings.min_vec_sim * (0.7 if (soft or is_cjk) else 1.0)
    bm25_thr = settings.min_bm25_score * (0.6 if (soft or is_cjk) else 1.0)

    # ä¹¦åå·çŸ­è¯­ï¼ˆå¦‚ã€Šæ´¥è²¼åŠæœå‹™å”è­°ã€‹ï¼‰ç”¨äºåŠ æƒ
    phrase_boost = 0.35 if is_cjk else 0.20
    m_phrase = re.search(r"ã€Š(.+?)ã€‹", query)
    phrase = m_phrase.group(1).strip() if m_phrase else None

    # è¯»å–ä¸€æ¬¡æƒ©ç½šè¡¨
    _PENALTY = None
    def _load_penalty():
        nonlocal _PENALTY
        if _PENALTY is None:
            from pathlib import Path
            p = Path("data/feedback/penalty.json")
            _PENALTY = json.loads(p.read_text("utf-8")) if p.exists() else {}
        return _PENALTY

    # é˜ˆå€¼è¿‡æ»¤ + èåˆ + phrase åŠ æƒ + æƒ©ç½š
    passed: List[Tuple[int, float]] = []
    pen = _load_penalty()
    for idx_i, sig in score_map.items():
        vec_ok = (sig["vec"] >= vec_thr)
        bm_ok = (sig["bm25"] >= bm25_thr)
        if not (vec_ok or bm_ok):
            continue

        # åŸºç¡€èåˆåˆ†ï¼ˆæ³¨æ„ï¼šFAISS D å·²æ˜¯ä½™å¼¦ï¼ŒBM25 æ˜¯åŸå§‹åˆ†ï¼‰
        combo = alpha_vec * max(sig["vec"], 0.0) + alpha_bm25 * max(sig["bm25"], 0.0)

        # ä¹¦åå·çŸ­è¯­å‘½ä¸­åŠ æƒ
        meta = index.meta[idx_i]
        meta_text = meta.get("text") or (index.texts[idx_i] if hasattr(index, "texts") and index.texts else "")
        if phrase and meta_text and phrase in meta_text:
            combo += phrase_boost

        # åº”ç”¨æƒ©ç½šï¼ˆğŸ‘åé¦ˆï¼‰
        from pathlib import Path
        key = f"{Path(meta['file']).name}::{meta.get('page')}"
        penalty = float(pen.get(key, 0.0))  # ä¾‹å¦‚ 0.15~0.30
        combo -= penalty

        passed.append((idx_i, combo))

    # æ’åº+æˆªæ–­
    passed.sort(key=lambda x: x[1], reverse=True)
    passed = passed[:k]

    # å‡ºç»“æœï¼šè¡¥é½ text å­—æ®µï¼Œä¾¿äºåç»­é€»è¾‘åˆ¤æ–­ä¸æ¸²æŸ“
    results: List[Dict] = []
    for idx_i, combo in passed:
        meta = index.meta[idx_i]
        text = meta.get("text") or (index.texts[idx_i] if hasattr(index, "texts") and index.texts else None)
        results.append({"text": text, "meta": meta, "idx": idx_i, "score": float(combo)})
    return results

# --- Prompt & Citations ---

from pathlib import Path
def build_prompt(messages: List[Dict], contexts: List[Dict]) -> List[Dict]:
    citation_blocks = []
    for i, c in enumerate(contexts, 1):
        file = Path(c["meta"]["file"]).name
        page = c["meta"].get("page")
        ref = f"[Source {i}] {file}" + (f", page {page}" if page is not None else "")
        raw = c["meta"].get("text") or ""
        snippet = raw[:1200]
        citation_blocks.append(f"{ref}\n\n{snippet}")

    K = len(contexts)
    system = {
        "role": "system",
        "content": (
            "You are ElderlyCare HK, a helpful assistant that answers strictly based on the provided Hong Kong Social Welfare Department documents.\n"
            f"- You are given exactly {K} sources. If you cite, you must use ONLY these tokens: "
            + ", ".join(f"[Source {i}]" for i in range(1, K+1)) + ".\n"
            "- Never invent new source indices. If information is not in the sources, clearly say it is not found.\n"
            "- Write plain text only (no JSON). Do NOT include a separate 'Sources' section.\n"
            "- If you cite inline, use the provided tokens verbatim (e.g., ... [Source 1]).\n"
            "- Treat each request as a fresh conversation and DO NOT use any memory beyond the messages provided in this request.\n"
            "- Resolve pronouns in the last user message using the chat history provided in this request.\n"
            "- If the user question is in Traditional Chinese, answer in Traditional Chinese.\n"
            "- If the question is in English, answer in English.\n"
        )
    }
    
    context_msg = {
        "role": "system",
        "content": "Relevant sources:\n\n" + "\n\n---\n\n".join(citation_blocks)
    }
    return [system, context_msg] + messages

def format_citations(contexts: List[Dict]) -> List[Dict]:
    from pathlib import Path
    seen = set()
    out = []
    for c in contexts:
        file = Path(c["meta"]["file"]).name
        page = c["meta"].get("page")
        key = (file, page)
        if key not in seen:
            seen.add(key)
            out.append({"file": file, "page": page, "snippet": None})
    return out
