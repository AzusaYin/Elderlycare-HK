import uvicorn
import numpy as np
import re, json, time, statistics, collections
from pathlib import Path
from typing import List, Dict, Tuple
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from .settings import settings
from .schemas import ChatRequest, ChatAnswer, IngestResponse, FeedbackIn
from .rag import Index, Embedder, hybrid_retrieve, build_prompt, format_citations, ingest_corpus
from .llm_client import smartcare_chat, smartcare_chat_stream, smartcare_translate_to_en
from .security import require_bearer
from .admin_docs import router as admin_docs_router

app = FastAPI(title="ElderlyCare HK â€” Backend")
app.include_router(admin_docs_router)

_LOG_DIR = Path("data/logs")
_LOG_DIR.mkdir(parents=True, exist_ok=True)
_USAGE_LOG = _LOG_DIR / "chat_usage.jsonl"
DEBUG_CLARIFY = True  # ä¸´æ—¶æ‰“å¼€ï¼›ç¡®è®¤æ— è¯¯åæ”¹ä¸º False

def _dbg(*a):
    if DEBUG_CLARIFY:
        print("[clarify]", *a)

def _append_jsonl_safe(path: Path, obj: dict):
    try:
        with path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    except Exception:
        pass

def _now_ms() -> int:
    return int(time.time() * 1000)

_FB_DIR = Path("data/feedback")
_FB_DIR.mkdir(parents=True, exist_ok=True)
_FB_JSONL = _FB_DIR / "feedback.jsonl"         # å·²å­˜åœ¨ï¼šä¸»åé¦ˆæ—¥å¿—
_FB_METRICS = _FB_DIR / "metrics.json"         # å·²å­˜åœ¨ï¼šèšåˆæŒ‡æ ‡
_FB_COUNTS = _FB_DIR / "penalty_counts.json"   # æ–°å¢ï¼šæŒ‰é¡µè®¡æ•° { (file,page): {up,down} }
_FB_PENALTY = _FB_DIR / "penalty.json"         # å·²å­˜åœ¨äº ragï¼›æˆ‘ä»¬ä¼šå³æ—¶ç”Ÿæˆ/æ›´æ–°

def _safe_read_json(path: Path, default):
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return default

def _write_json_atomic(path: Path, obj: dict):
    tmp = path.with_suffix(path.suffix + ".part")
    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    tmp.replace(path)

def _recompute_penalty_from_counts(counts: dict) -> dict:
    """
    æŠŠ { "file::page": {"up":x,"down":y}, ... } è½¬æˆ { "file::page": penalty_float }
    ç®€å•è§„åˆ™ï¼špenalty = min(1.0, BASE + STEP * max(0, down - up))
    ä½ ä¹Ÿå¯ä»¥æ¢æˆæ›´å¹³æ»‘çš„å‡½æ•°ï¼ˆä¾‹å¦‚ sigmoidï¼‰ã€‚
    """
    BASE, STEP = 0.10, 0.05   # å¯æŒ‰éœ€è¦å¾®è°ƒ
    out = {}
    for k, v in counts.items():
        up = int(v.get("up", 0))
        down = int(v.get("down", 0))
        excess = max(0, down - up)
        pen = min(1.0, BASE + STEP * excess)
        if pen > 0:
            out[k] = round(pen, 4)
    return out

def _bump_counts_and_penalty(recs: list[dict]):
    """
    æ ¹æ®è¿™æ¬¡åé¦ˆé‡Œé™„å¸¦çš„ citations æ‰¹é‡æ›´æ–° counts ä¸ penaltyã€‚
    recs: [{ "file": "...", "page": 12 }, ...]
    """
    counts = _safe_read_json(_FB_COUNTS, {})
    # æ¯æ¡ citation ç´¯è®¡ up/downï¼›ç”±è°ƒç”¨è€…å†³å®šå½“å‰åé¦ˆæ˜¯ up è¿˜æ˜¯ down
    def inc(key: str, field: str):
        row = counts.get(key) or {}
        row[field] = int(row.get(field, 0)) + 1
        counts[key] = row

    # ä¸Šå±‚ä¼šæŠŠ label é€ä¼ è¿›æ¥ï¼›è¿™é‡Œä¸åŒºåˆ†æ¯æ¡ citation çš„ label
    for r in recs:
        file = r.get("file")
        page = r.get("page")
        if not file:
            continue
        key = f"{file}::{page}"
        # å ä½ï¼Œå®é™…å¢é‡åœ¨è°ƒç”¨å¤„åšï¼ˆè§ feedback_in å†…ï¼‰
        counts.setdefault(key, counts.get(key, {"up": 0, "down": 0}))

    return counts, inc

_CITE_TAG_RE = re.compile(r"\[Source\s+(\d+)\]")
_PRONOUN_PAT = re.compile(r"\b(it|its|this|that|they|their)\b|[å®ƒå…¶é€™è©²]")
_CJK_RE = re.compile(r"[\u4e00-\u9fff]")  # åŸºæœ¬æ¼¢å­—

# ç®€å•åˆ«åæ˜ å°„ï¼ˆå¯ç»§ç»­è¡¥å……ï¼‰
ALIASES = {
    "OAA": "Old Age Allowance",
    "OALA": "Old Age Living Allowance",
    "LSG": "Lump Sum Grant",
    "LSGSS": "Lump Sum Grant Subvention System",
}

# ç”¨â€œéå­—æ¯æ•°å­—â€å‰åè§†å›¾æ›¿ä»£ \bï¼Œé¿å…åœ¨ä¸­æ–‡é‡Œå¤±æ•ˆ
# (?<![A-Za-z0-9]) â€¦â€¦ (?![A-Za-z0-9])
_ENTITY_EXTRACT_RE = re.compile(
    r"(?<![A-Za-z0-9])("  # ---- è‹±æ–‡æ­£å¼åç¨±ï¼ˆå¤šè©çŸ­èª + ç‰¹å®šå¾Œç¶´ï¼Œä¸åŒ¹é…å–®è©æœ¬èº«ï¼‰----
    # ä¾‹å¦‚ï¼šOld Age Allowance, Disability Allowance, Social Welfare Department Manual ...
    r"(?:Old\s+Age\s+Allowance|Old\s+Age\s+Living\s+Allowance|Disability\s+Allowance|"
    r"Comprehensive\s+Social\s+Security\s+Assistance|Lump\s+Sum\s+Grant\s+Subvention\s+System|"
    r"Lump\s+Sum\s+Grant|Infirmary\s+Care\s+Supplement|Foster\s+Care\s+Allowance|"
    r"Emergency\s+Foster\s+Care\s+Allowance)"
    r")(?![A-Za-z0-9])"
    r"|(?:OAA|OALA|CSSA|DA|LSG|LSGSS)"  # å¸¸è¦‹è‹±æ–‡ç¸®å¯«
    r"|(?:é•·è€…ç”Ÿæ´»æ´¥è²¼|é«˜é½¡æ´¥è²¼|è€å¹´æ´¥è²¼|å‚·æ®˜æ´¥è²¼|"
    r"ç¶œåˆç¤¾æœƒä¿éšœæ´åŠ©|è³‡åŠ©ç¦åˆ©æœå‹™|çµ±ä¸€æ’¥æ¬¾|æ•´ç­†æ’¥æ¬¾|æ•´ç­†æ’¥æ¬¾æ´¥åŠ©åˆ¶åº¦|çµ±ä¸€æ’¥æ¬¾åˆ¶åº¦|çµ±ä¸€æ’¥æ¬¾è³‡åŠ©|æ•´ç­†è³‡åŠ©|"
    r"è³‡åŠ©è¨ˆåŠƒ|æ´åŠ©è¨ˆåŠƒ|è£œåŠ©é‡‘|è£œè²¼|æ´¥è²¼è¨ˆåŠƒ|æ´¥è²¼å®‰æ’|"
    r"æ”¿ç­–|è¨ˆåŠƒ|æ‰‹å†Š|æŒ‡å¼•|é€šå‘Š|è¦ä¾‹|æ¢ä¾‹|æ–¹æ¡ˆ|åˆ¶åº¦|å®‰æ’)"
    , re.I
)

# é€™å€‹æ¯”ä¸Šé¢çš„ç¨çª„ï¼Œç”¨æ–¼ä½ çš„â€œå¯¦é«”æç¤º/æ¾„æ¸…â€æª¢æ¸¬ï¼ˆä¸éœ€è¦éå¤šå¹²æ“¾è©ï¼‰
# ä»…ç¤ºä¾‹ï¼Œè¯·å¹¶å…¥ä½ ç°æœ‰çš„ _ENTITY_PATï¼š
_ENTITY_PAT = re.compile(
    r"(?:"
    # ---- English concrete entities (no generic words here!) ----
    r"Old Age Living Allowance|OALA|Old Age Allowance|OAA|"
    r"Disability Allowance|DA|"
    r"Comprehensive Social Security Assistance|CSSA|"
    r"Lump Sum Grant Subvention System|LSG Subvention System|LSGSS|Lump Sum Grant|LSG|"
    r"Infirmary Care Supplement|Foster Care Allowance|Emergency Foster Care Allowance"
    r"|"
    # ---- ä¸­æ–‡å®ä½“ï¼ˆä¿ç•™ä½ ç°æœ‰ + æˆ‘ä»¬ä¹‹å‰è¡¥çš„ï¼‰----
    r"é•·è€…ç”Ÿæ´»æ´¥è²¼|é«˜é½¡æ´¥è²¼|è€å¹´æ´¥è²¼|å‚·æ®˜æ´¥è²¼|ç¶œåˆç¤¾æœƒä¿éšœæ´åŠ©|"
    r"æ•´ç­†æ’¥æ¬¾|æ•´ç­†æ’¥æ¬¾æ´¥åŠ©åˆ¶åº¦|çµ±ä¸€æ’¥æ¬¾åˆ¶åº¦|çµ±ä¸€æ’¥æ¬¾è³‡åŠ©|æ•´ç­†è³‡åŠ©|"
    r"è³‡åŠ©è¨ˆåŠƒ|æ´åŠ©è¨ˆåŠƒ|è£œåŠ©é‡‘|è£œè²¼|æ´¥è²¼è¨ˆåŠƒ|æ´¥è²¼å®‰æ’"
    r")",
    re.I,
)

# --- Helpers for zh-Hant queries ---
def _merge_dedup_hits(h1: list[dict], h2: list[dict], k: int) -> list[dict]:
    """ä»¥ (file, page, chunk_id) å»é‡ï¼›åˆ†æ•°å–è¾ƒå¤§å€¼ï¼›ä¿ç•™æ¥æºæ ‡è®°ä¾¿äºè°ƒè¯•"""
    seen = {}
    for src, tag in ((h1, "zh"), (h2, "en")):
        for h in (src or []):
            key = (h.get("file"), h.get("page"), h.get("chunk_id"))
            score = float(h.get("score", 0))
            if key not in seen or score > seen[key]["score"]:
                seen[key] = {**h, "score": score, "src": tag}
    return sorted(seen.values(), key=lambda x: x["score"], reverse=True)[:k]

def _expand_aliases_zh(q: str) -> str:
    """æŠŠç¹ä¸­çš„ä¿—ç§°/ç®€ç§°æ‰©æˆ (A OR B OR è‹±æ–‡å)ï¼›ä½ å¯æŠŠè¡¨æ…¢æ…¢è¡¥å……èµ·æ¥"""
    table = {
        "ç”Ÿæœé‡‘": ["é«˜é½¡æ´¥è²¼", "Old Age Allowance", "OAA"],
        "ç¶œæ´":   ["ç¶œåˆç¤¾æœƒä¿éšœæ´åŠ©", "Comprehensive Social Security Assistance", "CSSA"],

        # ğŸ”½ æ–°å¢ï¼šLSG/LSGSS åˆ«åæ—
        "æ•´ç­†æ’¥æ¬¾":           ["çµ±ä¸€æ’¥æ¬¾", "Lump Sum Grant", "LSG"],
        "æ•´ç­†æ’¥æ¬¾æ´¥åŠ©åˆ¶åº¦":   ["çµ±ä¸€æ’¥æ¬¾åˆ¶åº¦", "Lump Sum Grant Subvention System", "LSG Subvention System", "LSGSS", "LSG"],
        "çµ±ä¸€æ’¥æ¬¾åˆ¶åº¦":       ["Lump Sum Grant Subvention System", "LSG Subvention System", "LSGSS", "LSG"],
    }
    for k, vs in table.items():
        if k in q:
            q = q.replace(k, f"({ ' OR '.join([k] + vs) })")
    # å†å¥—ç”¨ä½ ç°æœ‰çš„è‹±æ–‡ç¼©å†™è¡¨
    for short, full in ALIASES.items():
        if short in q:
            q = q.replace(short, f"({short} OR {full})")
    return q

def _norm_for_entity(s: str) -> str:
    # å…¨è§’->åŠè§’ + å»æ‰å¤šé¤˜ç©ºç™½
    out = []
    for ch in s:
        code = ord(ch)
        if code == 0x3000: code = 0x20
        elif 0xFF01 <= code <= 0xFF5E: code -= 0xFEE0
        out.append(chr(code))
    return re.sub(r"\s+", " ", "".join(out)).strip()

def _extract_focus_phrase(s: str) -> str | None:
    s = _norm_for_entity(s or "")
    # ä¹¦åå·ä¼˜å…ˆï¼šå¦‚ã€Šæ´¥è²¼åŠæœå‹™å”è­°ã€‹
    m = re.search(r"ã€Š(.+?)ã€‹", s)
    if m: return m.group(1).strip()
    # é€€åŒ–ï¼šè¿ç»­å¤§å†™å¼€å¤´è¯ + å…³é”®å°¾è¯ï¼ˆAccounts/Allowance/Manual/...ï¼‰
    m = re.search(r"([A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,6}\s(?:Accounts?|Allowance|Manual|Programme|Scheme|Policy|System|Subvention))", s)
    return m.group(1).strip() if m else None

def _boost_by_phrase(contexts: list[dict], phrase: str | None, boost: float = 0.35) -> list[dict]:
    if not phrase: 
        return contexts
    for c in contexts:
        t = (c.get("text") 
             or (c.get("meta") or {}).get("text") 
             or "")
        if t and phrase in t:
            c["score"] = float(c.get("score", 0.0)) + boost
    return sorted(contexts, key=lambda x: float(x.get("score", 0.0)), reverse=True)

def _extract_entities_from_text(text: str) -> list[str]:
    if not text: 
        return []
    return [m.group(0).strip() for m in _ENTITY_EXTRACT_RE.finditer(text)]

def _suggest_entities_for(query: str, idx: "Index", emb: "Embedder", top_k: int = 5) -> list[str]:
    """
    ç”¨ç•¶å‰æŸ¥è©¢åœ¨ç´¢å¼•è£¡åšä¸€æ¬¡è¼•é‡æª¢ç´¢ï¼Œå¾å‘½ä¸­çš„ chunk æ–‡æœ¬ä¸­æŠ½å–å¯¦é«”åï¼ŒæŒ‰é »æ¬¡å»é‡æ’åºè¿”å›ã€‚
    """
    try:
        contexts = hybrid_retrieve(query, idx, emb, k=20, soft=True)
    except Exception:
        return []
    freq: dict[str, int] = {}
    for c in contexts:
        t = c["meta"].get("text") or ""
        for ent in _extract_entities_from_text(t):
            # åˆä½µå¤§å°å¯«/ç©ºç™½å·®ç•°
            key = re.sub(r"\s+", " ", ent).strip()
            freq[key] = freq.get(key, 0) + 1
    # æ’åº + å»æ‰éæ–¼ç± çµ±çš„è©
    bad = {"Allowance","Scheme","Manual","Programme","Grant","Service","Subvention","System","Policy","è¨ˆåŠƒ","æ´¥è²¼","æ‰‹å†Š","æ”¿ç­–"}
    items = [e for e,_ in sorted(freq.items(), key=lambda kv: kv[1], reverse=True) if e not in bad]
    # ä¿ç•™ä¸åŒå‰ç¶´çš„å‰ top_k å€‹
    out = []
    seen_lower = set()
    for e in items:
        low = e.lower()
        if low in seen_lower: 
            continue
        seen_lower.add(low)
        out.append(e)
        if len(out) >= top_k:
            break
    return out

def _looks_cjk(s: str) -> bool:
    return bool(_CJK_RE.search(s or ""))
# è½»é‡å¯å‘å¼å‚æ•°
_MIN_TOKENS = 3
_GENERIC_Q_RE = re.compile(
    r"^(what|how|why|tell me|can you|could you|explain|give me|i want to know|èªªèªª|ä»‹ç´¹|è§£é‡‹|è«‹è¬›è¬›|æˆ‘æƒ³çŸ¥é“)\b",
    re.I,
)

def _looks_specific(q: str, contexts: list[dict]) -> bool:
    """
    â€œå·²è¶³å¤Ÿå…·ä½“â€çš„åˆ¤å®šï¼š
      1) å‘½ä¸­å®ä½“æ­£åˆ™ï¼ˆæ”¿ç­–/æ´¥è´´/åˆ¶åº¦ç­‰å…·ä½“åï¼‰ã€‚
      2) å«ã€Šã€‹æ›¸åè™Ÿã€‚
      3) ä¸­æ–‡æŸ¥è©¢ï¼šCJK é•·åº¦ â‰¥ 6 ä¸” ä»¥å°ˆæœ‰å°¾è©çµå°¾ï¼ˆæ´¥è²¼/è£œåŠ©/æ’¥æ¬¾/åˆ¶åº¦/è¨ˆåŠƒ/æŒ‡å¼•/æœå‹™/æ´¥è²¼è¨ˆåŠƒ/è£œåŠ©é‡‘ï¼‰ã€‚
    ä¸å†ç”¨ã€Œä¸Šä¸‹æ–‡å‡ºç°è¯¥çŸ­è¯­ã€ä½œä¸ºä¾æ®ã€‚
    """
    qn = _norm_for_entity(q or "")
    if "ã€Š" in qn and "ã€‹" in qn:
        _dbg("looks_specific: booktitle")
        return True
    if _ENTITY_PAT.search(qn):
        _dbg("looks_specific: entity")
        return True

    s = _cjk_only(qn)
    if len(s) >= 6 and (
        any(s.endswith(w) for w in ("æ´¥è²¼", "è£œåŠ©", "æ’¥æ¬¾", "åˆ¶åº¦", "è¨ˆåŠƒ", "æŒ‡å¼•", "æœå‹™"))
        or s.endswith("æ´¥è²¼è¨ˆåŠƒ") or s.endswith("è£œåŠ©é‡‘")
    ):
        _dbg("looks_specific: zh length&suffix")
        return True

    _dbg("looks_specific: False")
    return False

def _tokenize_simple(s: str) -> list[str]:
    return [t for t in re.findall(r"\w+|[\u4e00-\u9fff]", s or "") if t.strip()]

def _is_ambiguous_heuristic(q: str) -> bool:
    """
    ä»…åœ¨éå¸¸â€œçŸ­/è™šâ€çš„æƒ…å†µä¸‹è¿”å› Trueï¼š
      - ç©º/å…¨æ˜¯ç©ºç™½
      - è‹±æ–‡é•¿åº¦ < 4 ä¸”å…¨æ˜¯é—®å¥è¯ (what/why/howç­‰)
      - ä¸­æ–‡ CJK é•¿åº¦ â‰¤ 2 çš„å•å­—é—®å¥
    ä¸å†å› ä¸ºåŒ…å«â€œæ”¿ç­–/æ´¥è²¼â€ç­‰è¯å°±ç›´æ¥åˆ¤æ³›ã€‚
    """
    if not q or not q.strip():
        return True

    s = q.strip()
    # è‹±æ–‡è¿‡çŸ­é—®å¥
    sl = s.lower()
    if len(sl) <= 3 and sl in {"?", "hi", "hey"}:
        return True

    # ä¸­æ–‡æçŸ­
    cz = _cjk_only(s)
    if cz and len(cz) <= 2:
        return True

    return False

# è¯­ä¹‰æ¨¡æ¿ï¼ˆæå°é›†åˆï¼Œæ•æ‰â€œæ³›é—®/æ‰©å†™/è§£é‡Šä¸€ä¸‹â€è¿™ç±»ï¼‰
_GENERIC_TEMPLATES = [
    "Tell me about it",
    "Tell me about this",
    "What is it",
    "Explain this",
    "Give me details",
    "I want to know more",
    "What about it",
    "è«‹ä»‹ç´¹ä¸€ä¸‹",
    "é€™æ˜¯ä»€éº¼",
    "èªªèªªçœ‹",
]

_GENERIC_EMB: np.ndarray | None = None  # æ‡’åŠ è½½ç¼“å­˜

def _ensure_generic_emb() -> np.ndarray:
    global _GENERIC_EMB
    if _GENERIC_EMB is None:
        emb = get_embedder()  # å¤ç”¨ç°æœ‰çš„ SentenceTransformerï¼ˆå·²åš normalizeï¼‰
        _GENERIC_EMB = emb.encode(_GENERIC_TEMPLATES)
        if _GENERIC_EMB.ndim == 1:
            _GENERIC_EMB = _GENERIC_EMB.reshape(1, -1)
    return _GENERIC_EMB

def _is_ambiguous_semantic(q: str, thr: float = 0.82) -> bool:
    q = (q or "").strip()
    if not q:
        return True
    emb = get_embedder()
    qe = emb.encode([q])[0]  # å·²å½’ä¸€åŒ–
    G = _ensure_generic_emb()  # (m, d)
    sims = (G @ qe)  # ä½™å¼¦ç›¸ä¼¼åº¦
    return float(np.max(sims)) >= thr

_GENERIC_HEADS_ZH = {"æ´¥è²¼", "æ”¿ç­–", "åˆ¶åº¦", "è¨ˆåŠƒ", "æ‰‹å†Š", "æœå‹™", "æŒ‡å¼•"}

def _cjk_only(s: str) -> str:
    return "".join(ch for ch in s if "\u4e00" <= ch <= "\u9fff")

def _contains_generic_head_zh(q: str) -> bool:
    """
    åªæœ‰ç•¶æŸ¥è©¢â€œå¾ˆçŸ­â€(â‰¤4å€‹ CJKå­—) ä¸” æ°å¥½æ˜¯å¸¸è¦‹æ³›è© æˆ– ä»¥æ³›è©çµå°¾(å¦‚â€œä»‹ç´¹æ´¥è²¼â€)æ™‚ï¼Œæ‰è¦–ç‚ºæ³›å•ã€‚
    ç›®çš„æ˜¯é¿å…æŠŠé•·è€Œå…·é«”çš„ä¸­æ–‡åè©çŸ­èªèª¤åˆ¤ç‚ºæ³›å•ã€‚
    """
    s = _cjk_only((q or "").strip())
    if not s:
        return False
    if len(s) <= 4 and (s in _GENERIC_HEADS_ZH or any(s.endswith(w) for w in _GENERIC_HEADS_ZH)):
        return True
    return False

# ===== EN generic heads =====
_GENERIC_HEADS_EN = {
    "allowance", "allowances",
    "policy", "policies",
    "scheme", "schemes",
    "program", "programs", "programme", "programmes",
    "service", "services",
    "manual", "handbook", "guideline", "guidelines",
}
_GENERIC_HEADS_EN_RE = re.compile(r"\b(" + "|".join(sorted(_GENERIC_HEADS_EN)) + r")\b", re.I)

def _contains_generic_head_en(q: str) -> bool:
    qn = _norm_for_entity(q or "").lower()
    qn = re.sub(r"[^a-z0-9\s]", " ", qn)  # å»æ ‡ç‚¹
    has_generic = bool(_GENERIC_HEADS_EN_RE.search(qn))
    ent = bool(_ENTITY_PAT.search(qn))    # æ³¨æ„æ­¤å¤„ _ENTITY_PAT å·²ä¸å« generic
    return has_generic and not ent

# ===== ZH generic heads (ä¿å®ˆ) =====
_GENERIC_HEADS_ZH = {"æ´¥è²¼", "æ”¿ç­–", "åˆ¶åº¦", "è¨ˆåŠƒ", "æ‰‹å†Š", "æœå‹™", "æŒ‡å¼•", "è£œåŠ©", "è£œè²¼"}

def _cjk_only(s: str) -> str:
    return "".join(ch for ch in s if "\u4e00" <= ch <= "\u9fff")

def _contains_generic_head_zh(q: str) -> bool:
    """
    åªæœ‰ç•¶æŸ¥è©¢â€œå¾ˆçŸ­â€(â‰¤4å€‹ CJKå­—) ä¸” æ°å¥½æ˜¯å¸¸è¦‹æ³›è© æˆ– ä»¥æ³›è©çµå°¾(å¦‚â€œä»‹ç´¹æ´¥è²¼â€)æ™‚æ‰è¦–ç‚ºæ³›å•ã€‚
    ç›®æ¨™ï¼šé¿å…å°‡é•·ä¸­æ–‡åè©çŸ­èªèª¤åˆ¤ç‚ºæ³›å•ã€‚
    """
    s = _cjk_only((q or "").strip())
    if not s:
        return False
    short = len(s) <= 4 and (s in _GENERIC_HEADS_ZH or any(s.endswith(w) for w in _GENERIC_HEADS_ZH))
    ent = bool(_ENTITY_PAT.search(_norm_for_entity(q or "")))
    _dbg("ZH heads:", short, "entity:", ent, "s=", s)
    return short and not ent

def _should_clarify_smart(user_query: str, lang: str | None = None) -> bool:
    """
    ç»¼åˆå¯å‘å¼ä¸è¯­ä¹‰åˆ¤æ–­ã€‚
    å¯¹ä¸­æ–‡ï¼ˆzh-Hantï¼‰ä½¿ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼ï¼Œå‡å°‘è¯¯åˆ¤ã€‚
    """
    # å¯å‘å¼åˆ¤æ–­ï¼šå¤ªçŸ­/å¤ªæ³›
    if _is_ambiguous_heuristic(user_query):
        _dbg("heuristic=True")
        return True

    # è¯­è¨€è‡ªé€‚åº”é˜ˆå€¼ï¼ˆä¸­æ–‡æ›´ä¸¥æ ¼ï¼‰
    thr = 0.82
    if lang == "zh-Hant":
        thr = 0.86

    amb = _is_ambiguous_semantic(user_query, thr=thr)
    _dbg("semantic:", amb, "thr:", thr)
    return amb

def _clarify_question(user_query: str, lang: str | None) -> str:
    """
    å½“æ£€æµ‹åˆ°ç”¨æˆ·é—®é¢˜æ¨¡ç³Šæ—¶ï¼Œç”Ÿæˆä¸€æ¡è¿½é—®å¥ï¼Œç”¨äºæç¤ºç”¨æˆ·å…·ä½“åŒ–é—®é¢˜ã€‚
    """
    if not user_query:
        user_query = "your question"

    # ä¸­æ–‡ç•Œé¢
    if lang == "zh-Hant":
        return (
            f"ä½ çš„å•é¡Œï¼ˆã€Œ{user_query}ã€ï¼‰ç›®å‰ç¯„åœéå¤§ã€‚"
            "æˆ‘åœ¨ç›¸é—œçš„å®˜æ–¹æ–‡æª”ä¸­æ‰¾ä¸åˆ°æœ‰é—œè©²ä¸»é¡Œçš„ä¿¡æ¯ã€‚"
        )

    # è‹±æ–‡ç•Œé¢
    return (
        f"Your question (â€œ{user_query}â€) is a bit broad. "
        "I couldn't find any information on that topic in documents related to elderly care."
    )

def _clarify_question_smart(user_query: str, lang: str | None, idx: "Index", emb: "Embedder") -> str:
    q = (user_query or "").strip()
    keywords = ["allowance", "policy", "scheme", "manual", "programme", "æ´¥è²¼", "æ”¿ç­–", "è¨ˆåŠƒ", "æ‰‹å†Š"]
    need_list = any(kw in q.lower() for kw in keywords) or _is_ambiguous_semantic(q)

    if need_list:
        cands = _suggest_entities_for(q, idx, emb, top_k=5)
        if cands:
            if lang == "zh-Hant":
                opts = "ã€".join(cands[:4])
                return f"ä½ çš„å•é¡Œè¼ƒç‚ºç± çµ±ï¼ˆã€Œ{q}ã€ï¼‰ã€‚ä½ æ˜¯åœ¨å• {opts}ï¼Œé‚„æ˜¯å…¶ä»–ï¼Ÿ"
            else:
                opts = ", ".join(cands[:4])
                return f'Your question (â€œ{q}â€) is a bit broad. Are you asking about {opts}, or something else?'

        # ğŸ”½ å…œåº•å€™é¸ï¼ˆæ²¡æœ‰æŠ½åˆ°å®ä½“æ—¶ä»ç„¶ç»™å‡ºå…¸å‹é€‰é¡¹ï¼‰
        fallback_en = ["Old Age Allowance (OAA)", "Old Age Living Allowance (OALA)", "Disability Allowance (DA)"]
        fallback_zh = ["é«˜é½¡æ´¥è²¼ï¼ˆOAAï¼‰", "é•·è€…ç”Ÿæ´»æ´¥è²¼ï¼ˆOALAï¼‰", "å‚·æ®˜æ´¥è²¼ï¼ˆDAï¼‰"]
        if lang == "zh-Hant":
            return f"ä½ çš„å•é¡Œè¼ƒç‚ºç± çµ±ï¼ˆã€Œ{q}ã€ï¼‰ã€‚ä½ æ˜¯åœ¨å• {fallback_zh[0]}ã€{fallback_zh[1]} æˆ– {fallback_zh[2]}ï¼Œé‚„æ˜¯å…¶ä»–ï¼Ÿ"
        else:
            return f'Your question (â€œ{q}â€) is a bit broad. Are you asking about {fallback_en[0]}, {fallback_en[1]}, or {fallback_en[2]}?'

    # å€™é¸ç©ºä¸”ä¸éœ€è¦åˆ—è¡¨æ™‚ï¼Œé€€å›é€šç”¨æç¤º
    return _clarify_question(user_query, lang)

def _expand_aliases(text: str) -> str:
    out = text
    for k, v in ALIASES.items():
        out = re.sub(rf"\b{k}\b", v, out, flags=re.I)
    return out

def _guess_entity_from_history(msgs: list[dict]) -> str | None:
    """å¾å°è©±æ­·å²ä¸­çŒœæ¸¬æœ€è¿‘å‡ºç¾çš„æ˜ç¢ºæ”¿ç­–/æ´¥è²¼åï¼ˆæ”¯æ´ç¹ä¸­èˆ‡å…¨è§’ï¼‰"""
    for m in reversed(msgs):
        txt = _expand_aliases((m.get("content") or "").strip())
        if not txt:
            continue
        # ğŸ”¹åœ¨åŒ¹é…å‰åšæ­£è¦åŒ–
        normed = _norm_for_entity(txt)
        hit = _ENTITY_PAT.search(normed)
        if hit:
            return hit.group(0)

    # è‹¥ä»æœªå‘½ä¸­ï¼Œé€€å›é¦–è¼ªè¨Šæ¯å†è©¦
    for m in msgs:
        txt = _expand_aliases((m.get("content") or "").strip())
        normed = _norm_for_entity(txt)
        hit = _ENTITY_PAT.search(normed)
        if hit:
            return hit.group(0)
    return None

_FEEDBACK_DIR = Path("data/feedback")
_FEEDBACK_DIR.mkdir(parents=True, exist_ok=True)
_FEEDBACK_PATH = _FEEDBACK_DIR / "feedback.jsonl"
_METRICS_PATH  = _FEEDBACK_DIR / "metrics.json"   # ç”¨äºæç®€åœ¨çº¿æŒ‡æ ‡

def _append_jsonl(path: Path, obj: dict):
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _bump_metrics(label: str):
    # è¶…è¿·ä½ åœ¨çº¿æŒ‡æ ‡ï¼šç´¯è®¡ up/down æ¬¡æ•°ï¼›å¯æ‰©å±•ä¸ºåˆ†æ¡¶(ç›¸ä¼¼åº¦åŒºé—´)ç»Ÿè®¡
    m = {"up": 0, "down": 0}
    if _METRICS_PATH.exists():
        try: m = json.loads(_METRICS_PATH.read_text(encoding="utf-8"))
        except Exception: pass
    m[label] = m.get(label, 0) + 1
    _METRICS_PATH.write_text(json.dumps(m), encoding="utf-8")

def _iter_feedback_jsonl():
    if not _FB_JSONL.exists():
        return
    for line in _FB_JSONL.read_text(encoding="utf-8").splitlines():
        if not line.strip():
            continue
        try:
            yield json.loads(line)
        except Exception:
            continue

import datetime as _dt
def _iter_usage():
    if not _USAGE_LOG.exists():
        return
    for line in _USAGE_LOG.read_text(encoding="utf-8").splitlines():
        if not line.strip(): 
            continue
        try:
            yield json.loads(line)
        except Exception:
            continue

@app.get("/metrics/usage")
def metrics_usage(days: int = 7, _auth=Depends(require_bearer)):
    """
    åŸºç¡€ä½¿ç”¨ç»Ÿè®¡ï¼šæœ€è¿‘ N å¤©ï¼ˆé»˜è®¤ 7ï¼‰
    è¿”å›æ€»è¯·æ±‚æ•° / clarifications / æ— å‘½ä¸­ / å¹³å‡ä¸P95æ—¶å»¶ / å…³é”®è¯Top-Nï¼ˆè‹±æ–‡/æ•°å­—ç®€å•åˆ‡è¯ï¼‰
    """
    now_ms = _now_ms()
    cutoff = now_ms - int(days) * 24 * 60 * 60 * 1000

    total = 0
    clarified = 0
    nohit = 0
    found = 0
    durs = []
    ans_lens = []
    word_counter = collections.Counter()
    per_day = collections.Counter()  # YYYY-MM-DD -> count

    for r in _iter_usage() or []:
        ts = r.get("ts")
        if isinstance(ts, (int, float)) and ts < cutoff:
            continue
        total += 1
        if r.get("clarified"): clarified += 1
        if not r.get("found"): nohit += 1
        else: found += 1
        dur = r.get("duration_ms")
        if isinstance(dur, (int, float)) and dur >= 0:
            durs.append(float(dur))
        al = r.get("answer_len")
        if isinstance(al, int):
            ans_lens.append(al)
        # ç®€å•å…³é”®è¯ï¼ˆè‹±æ–‡/æ•°å­—ï¼‰ï¼Œå¦‚éœ€ä¸­è‹±æ–‡æ··åˆå¯åœ¨æ­¤å¤„æ›¿æ¢ä¸ºæ›´å¥½çš„åˆ†è¯é€»è¾‘
        q = r.get("user_query") or ""
        for w in re.findall(r"[A-Za-z0-9]{3,}", q):
            word_counter[w.lower()] += 1

        # day bucket
        if isinstance(ts, (int, float)):
            day = _dt.datetime.utcfromtimestamp(ts/1000.0).strftime("%Y-%m-%d")
            per_day[day] += 1

    avg_dur = statistics.mean(durs) if durs else None
    p95_dur = (statistics.quantiles(durs, n=20)[18] if len(durs) >= 20 else (max(durs) if durs else None))
    avg_ans = statistics.mean(ans_lens) if ans_lens else None

    top_keywords = [{"term": k, "count": v} for k, v in word_counter.most_common(20)]
    series = [{"day": d, "count": c} for d, c in sorted(per_day.items())]

    return {
        "window_days": int(days),
        "total": total,
        "found": found,
        "nohit": nohit,
        "clarified": clarified,
        "avg_duration_ms": round(avg_dur, 2) if avg_dur is not None else None,
        "p95_duration_ms": round(p95_dur, 2) if p95_dur is not None else None,
        "avg_answer_len": round(avg_ans, 2) if avg_ans is not None else None,
        "top_keywords": top_keywords,
        "daily_series": series,
        "updated": now_ms
    }

@app.get("/feedback/metrics")
def feedback_metrics(days: int = 7, _auth=Depends(require_bearer)):
    """
    è¿”å›æœ€è¿‘ N å¤©çš„ up/down ç»Ÿè®¡ä¸ up-rateã€‚days é»˜è®¤ 7ã€‚
    æ•°æ®æºï¼šdata/feedback/feedback.jsonl
    """
    now_ms = int(time.time() * 1000)
    cutoff_ms = now_ms - int(days) * 24 * 60 * 60 * 1000

    up = down = total = 0
    for rec in _iter_feedback_jsonl() or []:
        ts = rec.get("ts")  # æˆ‘ä»¬å†™å…¥çš„æ˜¯æ¯«ç§’æ—¶é—´æˆ³
        if isinstance(ts, (int, float)):
            if ts < cutoff_ms:
                continue
        # æ— æ—¶é—´æˆ³çš„è€æ•°æ®ï¼šå¯ä»¥é€‰æ‹©çº³å…¥æˆ–è·³è¿‡ï¼›è¿™é‡Œçº³å…¥
        if rec.get("label") == "up":
            up += 1
        elif rec.get("label") == "down":
            down += 1
        total += 1

    uprate = (up / total) if total > 0 else 0.0
    return {
        "days": int(days),
        "up": up,
        "down": down,
        "total": total,
        "uprate": round(uprate, 4),
        "updated": now_ms
    }

@app.post("/feedback")
async def feedback_in(body: FeedbackIn, _auth=Depends(require_bearer)):
    rec = body.model_dump()
    rec["ts"] = int(time.time() * 1000)

    # 1) å…ˆå†™ä¸»æ—¥å¿— & èšåˆè®¡æ•°ï¼ˆä¸ä½ ç°æœ‰é€»è¾‘ä¸€è‡´ï¼‰
    try:
        _append_jsonl(_FEEDBACK_PATH, rec)
        _bump_metrics(body.label)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"write feedback failed: {e}")

    # 2) å³æ—¶æŠŠè¿™æ¡åé¦ˆâ€œè½¬è¯‘â€ä¸ºé¡µçº§ counts & penaltyï¼Œç«‹åˆ»ç”Ÿæ•ˆ
    try:
        # è½½å…¥ç°æœ‰ countsï¼Œå¹¶å¾—åˆ°ä¸€ä¸ªç´¯åŠ å™¨ inc()
        counts, inc = _bump_counts_and_penalty(rec.get("citations") or [])
        # å¯¹æœ¬æ¡åé¦ˆå¯¹åº”çš„æ‰€æœ‰ citation åš up/down å¢é‡
        fld = "up" if body.label == "up" else "down"
        for c in rec.get("citations") or []:
            file = c.get("file")
            page = c.get("page")
            if not file:
                continue
            key = f"{file}::{page}"
            # åˆå§‹åŒ–å¦‚æœä¸å­˜åœ¨
            if key not in counts:
                counts[key] = {"up": 0, "down": 0}
            counts[key][fld] = int(counts[key].get(fld, 0)) + 1

        # å†™å› counts
        _write_json_atomic(_FB_COUNTS, counts)

        # è®¡ç®—/å†™å› penalty.json â€”â€” rag.hybrid_retrieve ä¼šåœ¨ä¸‹ä¸€æ¬¡æ£€ç´¢æ—¶è¯»å–
        penalty = _recompute_penalty_from_counts(counts)
        _write_json_atomic(_FB_PENALTY, penalty)
    except Exception as e:
        # ä¸ä¸­æ–­ä¸»æµç¨‹ï¼›åªæ˜¯è®°å½•è¿™æ¬¡â€œå³æ—¶æƒ©ç½šæ›´æ–°â€å¤±è´¥
        print(f"[WARN] feedback penalty update failed: {e}")

    return {"ok": True}

@app.middleware("http")
async def add_pna_header(request: Request, call_next):
    # è®© Chrome çš„ Private Network Access é¢„æ£€é€šè¿‡
    response: Response = await call_next(request)
    response.headers["Access-Control-Allow-Private-Network"] = "true"
    return response

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

_index: Index | None = None
_embedder: Embedder | None = None

def _extract_used_indices(answer_text: str, max_k: int) -> List[int]:
    """ä»ç­”æ¡ˆæ­£æ–‡é‡Œæå–å®é™…è¢«ä½¿ç”¨çš„ [Source n]ï¼Œå»é‡å¹¶æŒ‰é¦–æ¬¡å‡ºç°é¡ºåºè¿”å›ã€‚"""
    seen = set()
    order: List[int] = []
    for m in _CITE_TAG_RE.finditer(answer_text or ""):
        try:
            n = int(m.group(1))
        except Exception:
            continue
        if 1 <= n <= max_k and n not in seen:
            seen.add(n)
            order.append(n)
    return order

def _citations_by_usage(answer_text: str, contexts: List[Dict]) -> List[Dict]:
    """åªè¿”å›æ­£æ–‡é‡Œå®é™…ä½¿ç”¨åˆ°çš„ [Source n] æ‰€å¯¹åº”çš„ citationsã€‚"""
    from .rag import format_citations
    all_cites = format_citations(contexts)  # é¡ºåºä¸ [Source 1..K] å¯¹åº”
    used = _extract_used_indices(answer_text, len(all_cites))
    return [all_cites[i - 1] for i in used]  # i ä» 1 å¼€å§‹

def _sanitize_inline_citations(text: str, max_k: int) -> str:
    def repl(m):
        n = int(m.group(1))
        return m.group(0) if 1 <= n <= max_k else ""
    return _CITE_TAG_RE.sub(repl, text)

def get_index() -> Index:
    global _index
    if _index is None:
        idx = Index(settings.index_dir)
        idx.load()
        if idx.faiss is None:
            raise HTTPException(status_code=503, detail="Index not built yet. Run /ingest or scripts/ingest.py")
        _index = idx
    return _index

def get_embedder() -> Embedder:
    global _embedder
    if _embedder is None:
        _embedder = Embedder(settings.embedding_model_name, settings.embedding_device)
    return _embedder

@app.get("/healthz")
async def healthz():
    return {"status": "ok"}

@app.post("/ingest", response_model=IngestResponse)
async def ingest(_auth=Depends(require_bearer)):
    docs_count, chunks_count = ingest_corpus(settings.docs_dir, settings.index_dir)
    global _index, _embedder
    _index = None
    _embedder = None
    return IngestResponse(documents_indexed=docs_count, chunks_indexed=chunks_count)

def _extract_answer_text(data: dict) -> str:
    # æ–°å¢ï¼šå¤„ç† "answer" æ˜¯åµŒå¥—å­—ç¬¦ä¸² JSON çš„æƒ…å†µ
    if "answer" in data and isinstance(data["answer"], str):
        try:
            parsed = json.loads(data["answer"].replace("'", '"'))  # å…¼å®¹å•å¼•å·
            if isinstance(parsed, dict) and "response" in parsed:
                return parsed["response"]
        except Exception:
            pass  # fallback below

    try:
        return data["choices"][0]["message"]["content"]
    except Exception:
        pass
    if isinstance(data, dict) and "choices" in data and data["choices"]:
        c0 = data["choices"][0]
        if isinstance(c0, dict) and "text" in c0 and isinstance(c0["text"], str):
            return c0["text"]
    for key in ("content", "answer", "message", "data"):
        v = data.get(key)
        if isinstance(v, str) and v.strip():
            return v
        if isinstance(v, dict):
            for kk in ("content", "text", "answer"):
                vv = v.get(kk)
                if isinstance(vv, str) and vv.strip():
                    return vv
    return str(data)

def _normalize_answer_text(text: str) -> str:
    if not isinstance(text, str):
        return str(text)

    # 1) è§£æ {"response": "..."} æˆ– {'response': '...'}
    try:
        obj = json.loads(text)
        if isinstance(obj, dict) and isinstance(obj.get("response"), str):
            text = obj["response"]
    except Exception:
        m = re.match(r"\s*\{[^}]*'response'\s*:\s*'(.*?)'\s*\}\s*$", text, flags=re.S)
        if m:
            text = m.group(1)

    # 2) å»æ‰æ–‡æœ«å†…åµŒçš„ Sources æ®µ
    text = re.sub(r"\n+Sources\s*\n(?:\[[^\n]+\].*\n?)+\s*$", "", text, flags=re.I)

    # 3) ç»Ÿä¸€æ¢è¡Œï¼Œå¹¶æŠŠå­—é¢é‡ \\n è½¬æˆçœŸæ­£æ¢è¡Œ
    text = text.replace("\r\n", "\n")
    if "\\n" in text:
        text = text.replace("\\n", "\n")

    return text.strip()

def _extract_stream_token_preserve(text_line: str) -> str | None:
    line = text_line
    if not line:
        return None

    # å¤„ç† SSE å‰ç¼€
    s = line.strip()
    if s.lower() == "[done]":
        return None
    if s.startswith("data:"):
        s = s[5:].strip()

    # å°è¯•è§£æ JSONï¼›å¦‚æœä¸æ˜¯ JSONï¼Œç›´æ¥è¿”å›åŸè¡Œï¼ˆä¸æ”¹åŠ¨ï¼‰
    try:
        obj = json.loads(s)
    except Exception:
        return line  # é JSONï¼ŒåŸæ ·è¿”å›ï¼ˆä¿ç•™å…¶ä¸­æ¢è¡Œ/ç©ºæ ¼ï¼‰

    # åœ¨å¸¸è§è·¯å¾„ä¸­å–å­—ç¬¦ä¸²å€¼ï¼ˆåŸæ ·è¿”å›ï¼‰
    def get_path(o, path):
        cur = o
        for p in path:
            if isinstance(p, int):
                if isinstance(cur, list) and len(cur) > p:
                    cur = cur[p]
                else:
                    return None
            else:
                if isinstance(cur, dict) and p in cur:
                    cur = cur[p]
                else:
                    return None
        return cur if isinstance(cur, str) else None

    candidates = [
        ("choices", 0, "delta", "content"),
        ("choices", 0, "text"),
        ("response",),
        ("content",),
        ("answer",),
        ("message", "content"),
        ("data", "content"),
    ]
    for path in candidates:
        val = get_path(obj, path)
        if isinstance(val, str):
            return val  # åŸæ ·è¿”å›ï¼Œä¸åšä»»ä½•æ›¿æ¢æˆ–å»é™¤

    # æ²¡å‘½ä¸­å°±æŠŠæ•´è¡Œ JSON ä¸¢å¼ƒï¼ˆé¿å…å†æŠŠ JSON æ–‡æœ¬å›ä¼ å‰ç«¯ï¼‰
    return None

def _extract_stream_piece(line: str) -> str:
    """
    ä» SmartCare æµå¼æ¯è¡Œé‡Œæå–çº¯æ–‡æœ¬ã€‚
    å…¼å®¹ä»¥ä¸‹æƒ…å†µï¼š
      - {"response": " ... "}
      - {"text": "..."} / {"content": "..."} / {"data": "..."}
      - çº¯æ–‡æœ¬ï¼ˆç›´æ¥è¿”å›ï¼‰
      - SSE é£æ ¼çš„ 'data: {...}'ï¼ˆå¯é€‰å…¼å®¹ï¼‰
    """
    if not line:
        return ""

    # 1) å»æ‰å¯èƒ½çš„ SSE å‰ç¼€
    if line.startswith("data:"):
        line = line[len("data:"):].strip()

    # 2) ä¼˜å…ˆ JSON è§£æ
    try:
        obj = json.loads(line)
        if isinstance(obj, dict):
            for key in ("response", "text", "content", "data"):
                val = obj.get(key)
                if isinstance(val, str):
                    return val
        # å¦‚æœæ˜¯æ•°ç»„æˆ–å…¶ä»–ç»“æ„ï¼Œè¿™é‡Œä¸å¤„ç†
    except Exception:
        pass

    # 3) å…¼å®¹éä¸¥æ ¼ JSONï¼Œç®€å•ç”¨æ­£åˆ™å…œåº•ï¼ˆä¾‹å¦‚ {'response': '...'}ï¼‰
    m = re.search(r"'response'\s*:\s*'(.*?)'", line)
    if m:
        return m.group(1)

    # 4) å¦‚æœ line ä¸æ˜¯ JSONï¼Œå°±å½“ä½œçº¯æ–‡æœ¬ï¼ˆå°‘è§ä½†å®‰å…¨ï¼‰
    #    ä½†è¦æ’é™¤çº¯ç²¹çš„ç©ºç™½/å¿ƒè·³
    if line.strip():
        return line
    return ""

def _not_found_text(lang: str | None) -> str:
    if lang == "zh-Hant":
        return ("æŠ±æ­‰ï¼Œæˆ‘åœ¨ç›®å‰ç´å…¥çš„ç¤¾æœƒç¦åˆ©æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°ä½ é€™å€‹å•é¡Œçš„å…·é«”ç­”æ¡ˆã€‚"
                "ä½ å¯ä»¥å˜—è©¦ï¼š\n"
                "â€¢ æ›ä¸€ç¨®èªªæ³•æˆ–è£œå……æ›´å…·é«”çš„åè©ï¼ˆä¾‹å¦‚æ´¥è²¼åç¨±ã€æœå‹™å–®ä½ï¼‰\n"
                "â€¢ æŒ‡å®šæ–‡ä»¶æˆ–å¹´ä»½ï¼ˆä¾‹å¦‚ 2024 å¹´ LSG Subvention Manualï¼‰\n"
                "å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å¹«ä½ é‡è¿°æŸ¥è©¢æˆ–åˆ—å‡ºç›¸é—œç« ç¯€ä¾›ä½ æŸ¥é–±ã€‚")
    return ("Sorry, I couldn't find a specific answer to this question in the indexed documents.\n"
            "You can try:\n"
            "â€¢ Rephrasing or adding more specific terms (e.g., the exact allowance name/service unit)\n"
            "â€¢ Mentioning a document or year (e.g., 2024 LSG Subvention Manual)\n"
            "If you like, I can help refine your query or show nearby sections.")

from .llm_client import smartcare_chat, smartcare_chat_stream, smartcare_rewrite_query
@app.post("/chat", response_model=ChatAnswer)
async def chat(req: ChatRequest, _auth=Depends(require_bearer)):
    idx = get_index()
    emb = get_embedder()

    # 1) å–å¾— messagesï¼ˆå‰ç«¯ç°åœ¨ä¼šå¸¦æœ€è¿‘è‹¥å¹²è½®ï¼‰
    msgs = [m.dict() if hasattr(m, "dict") else m for m in req.messages]

    # 2) æ”¹å†™ä¸ºç‹¬ç«‹é—®é¢˜ï¼ˆæœ‰å†å²æ—¶æ›´æœ‰æ•ˆï¼‰
    #    å¦‚æœ messages å¾ˆçŸ­ï¼ˆåªæœ‰ä¸€æ¡ userï¼‰ï¼Œå°±ç›´æ¥ç”¨å®ƒ
    if len(msgs) >= 2:
        rewritten = await smartcare_rewrite_query(msgs)
        user_query = rewritten
    else:
        user_query = msgs[-1]["content"]

    # â˜… è¦å‰‡å…œåº•ï¼šè‹¥ä»å«ä»£è©ï¼Œå˜—è©¦è£œä¸Šæœ€è¿‘å¯¦é«”
    if _PRONOUN_PAT.search(user_query):
        ent = _guess_entity_from_history(msgs)
        if ent:
            # ä¸ç²—æš´æ›¿æ¢ç”¨æˆ·åŸå¥ï¼Œåªç»™æ£€ç´¢ä¿¡å·åŠ æ³¨é‡Š
            user_query = f"{user_query} (about {ent})"
    
    # 3) ç”¨æ”¹å†™åçš„ç‹¬ç«‹é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼ˆç¹ä¸­ï¼šåŸæ–‡ + è‹±è¯‘ åŒé€šé“åˆå¹¶ï¼‰
    is_followup_pronoun = bool(_PRONOUN_PAT.search(msgs[-1]["content"]))

    if req.language == "zh-Hant":
        # 3.1 åˆ«å/ä¿—ç§°æ‰©å±•ï¼ˆåªå½±å“æ£€ç´¢ï¼Œä¸æ”¹åŠ¨åŸ messagesï¼‰
        query_zh = _expand_aliases_zh(user_query)

        # 3.2 åŒæ—¶ç”¨ç¹ä¸­ä¸è‹±è¯‘æ£€ç´¢
        try:
            query_en = await smartcare_translate_to_en(query_zh)
        except Exception:
            query_en = None

        hits_zh = hybrid_retrieve(query_zh, idx, emb, settings.top_k, soft=is_followup_pronoun)
        hits_en = hybrid_retrieve(query_en, idx, emb, settings.top_k, soft=True) if query_en else []

        # 3.3 åˆå¹¶å»é‡ï¼ˆä¿ç•™è¾ƒé«˜åˆ†ï¼Œæˆªåˆ° top_kï¼‰
        contexts = _merge_dedup_hits(hits_zh, hits_en, settings.top_k)

        # ï¼ˆå¯é€‰ï¼‰æŠŠâ€œå®é™…ç”¨äºæ£€ç´¢çš„ queryâ€è®°ä¸‹æ¥ä¾¿äºæ—¥å¿—æ’æŸ¥
        user_query = query_zh
    else:
        contexts = hybrid_retrieve(user_query, idx, emb, settings.top_k, soft=is_followup_pronoun)

    # ä»â€œå½“å‰é—®å¥â€å’Œâ€œä¸Šä¸€ä¸ªé—®å¥/å›ç­”â€ä¸­æŠ“ä¸€ä¸ªç„¦ç‚¹çŸ­è¯­
    focus = (_extract_focus_phrase(msgs[-1]["content"]) or
            (len(msgs) >= 2 and _extract_focus_phrase(msgs[-2]["content"])) or
            _guess_entity_from_history(msgs))

    # æŒ‰ç„¦ç‚¹çŸ­è¯­é‡æ’ï¼ˆæŠŠåŒ…å«è¯¥çŸ­è¯­çš„åˆ†ç‰‡å¾€å‰æ¨ï¼‰
    contexts = _boost_by_phrase(contexts, focus, boost=0.35)

    # === æ£€ç´¢ä¿¡å·è¯„ä¼° ===
    max_score = max((c.get("score", 0.0) for c in contexts), default=0.0)
    found_enough = len(contexts) >= settings.min_sources_required
    # æ£€ç´¢â€œå¼±å‘½ä¸­â€çš„åˆ¤å®šé˜ˆå€¼ï¼ˆå¯åœ¨ .env æš´éœ²ï¼Œå¦‚ CLARIFY_RETRIEVE_THRï¼‰
    retrieve_weak_thr = getattr(settings, "clarify_retrieve_thr", settings.min_vec_sim * 0.4)
    is_weak = max_score < retrieve_weak_thr

    # === åªæœ‰åœ¨â€œå‘½ä¸­ä¸è¶³æˆ–è¿‡å¼±â€æ—¶ï¼Œæ‰è€ƒè™‘æ¾„æ¸… ===
    want_clarify = False
    if (not found_enough) or is_weak:
        try:
            # æ¨èï¼š_should_clarify_smart æ”¯æŒ contexts / thr å‚æ•°
            want_clarify = _should_clarify_smart(
                user_query, req.language, contexts=contexts,
                thr=getattr(settings, "clarify_thr", 0.65)
            )
        except TypeError:
            # å…¼å®¹æ—§ç­¾å
            want_clarify = _should_clarify_smart(user_query, req.language)

        # è¯­è¨€æ³›è¯çš„è½»é‡è§„åˆ™ï¼ˆè‹±æ–‡ â€œallowance/policy/schemeâ€¦â€ï¼Œä¸­æ–‡ä¿å®ˆæ³›è¯ï¼‰
        if req.language == "en":
            if _contains_generic_head_en(user_query):
                # è‹±æ–‡å‘½ä¸­æ³›è¯æ—¶æ›´å€¾å‘äºæ¾„æ¸…
                want_clarify = True
        else:
            if _contains_generic_head_zh(user_query) and not _looks_specific(user_query, contexts):
                want_clarify = True

        if want_clarify:
            text = _clarify_question_smart(user_query, req.language, idx, emb)
            if req.stream:
                async def event_stream():
                    yield text
                    yield "\nCITATIONS:[]\n"
                return StreamingResponse(event_stream(), media_type="text/plain")
            else:
                return ChatAnswer(answer=text, citations=[])

    # èµ°åˆ°è¿™é‡Œï¼šæ£€ç´¢å¹¶ä¸å¼±ï¼ˆæˆ–è™½å¼±ä½†ä¸éœ€æ¾„æ¸…ï¼‰
    # å¦‚æœçœŸçš„å•¥éƒ½æ²¡æœ‰ï¼Œå†èµ°â€œæœªæ‰¾åˆ°â€
    if not found_enough:
        text = _not_found_text(req.language)
        if req.stream:
            async def event_stream():
                yield text
                yield "\nCITATIONS:[]\n"
            return StreamingResponse(event_stream(), media_type="text/plain")
        else:
            return ChatAnswer(answer=text, citations=[])

    # 4) æ­£å¸¸æ‹¼ promptï¼ˆæŠŠåŸ messages å‘ç»™æ¨¡å‹ï¼Œè¿™æ ·å®ƒèƒ½â€œæŒ‰ä¸Šä¸‹æ–‡å£å»â€å›ç­”ï¼‰
    prompt_msgs = build_prompt(msgs, contexts)
    if req.language == "zh-Hant":
        prompt_msgs.insert(0, {
            "role": "system",
            "content": (
                "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œ"
                "èªæ°£è¦ªåˆ‡ã€å¥å­ç°¡çŸ­ï¼Œé¿å…ä½¿ç”¨è‰±æ·±è©å½™ï¼Œ"
                "è®“é•·è€…èƒ½å®¹æ˜“æ˜ç™½ã€‚"
            )
        })
    elif req.language == "en":
        prompt_msgs.insert(0, {
            "role": "system",
            "content": (
                "Please answer in clear, short English sentences within 100 words suitable for older adults. "
                "Avoid jargon and keep the response friendly."
            )
        })

    if req.stream:
        async def event_stream():
            buffer = ""
            async for line in smartcare_chat_stream(prompt_msgs):
                piece = _extract_stream_piece(line)
                if piece:
                    buffer += piece
                    yield piece
            clean_final = _sanitize_inline_citations(buffer, len(contexts))
            cites = _citations_by_usage(clean_final, contexts)
            trailer = "CITATIONS:" + json.dumps(cites, ensure_ascii=False)
            yield "\n" + trailer + "\n"

            # âœ… å‘é€å®Œæ¯•åè®°å½• usage æ—¥å¿—ï¼ˆæ­¤æ—¶ clean_final/cites å¯ç”¨ï¼‰
            try:
                _append_jsonl_safe(_USAGE_LOG, {
                    "ts": _now_ms(),
                    "lang": req.language,
                    "duration_ms": None,
                    "user_query": user_query,
                    "clarified": False,  # èµ°åˆ°è¿™é‡Œä»£è¡¨æ²¡æœ‰æ¾„æ¸…ç›´æ¥å›ç­”
                    "context_hits": len(contexts),
                    "found": len(contexts) >= settings.min_sources_required,
                    "answer_len": len(clean_final or ""),
                    "citations_cnt": len(cites or []),
                    "stream": True
                })
            except Exception:
                pass

        return StreamingResponse(event_stream(), media_type="text/plain")

    data = await smartcare_chat(prompt_msgs)
    answer_text = _normalize_answer_text(_extract_answer_text(data))
    answer_text = _sanitize_inline_citations(answer_text, len(contexts))
    citations = _citations_by_usage(answer_text, contexts)

    try:
        _append_jsonl_safe(_USAGE_LOG, {
            "ts": _now_ms(),
            "lang": req.language,
            "duration_ms": None,
            "user_query": user_query,
            "clarified": _should_clarify_smart(user_query) and not _looks_specific(user_query, contexts),
            "context_hits": len(contexts),
            "found": len(contexts) >= settings.min_sources_required,
            "answer_len": len(answer_text or ""),
            "citations_cnt": len(citations or []),
            "stream": False
        })
    except Exception:
        pass

    return ChatAnswer(answer=answer_text, citations=citations)

if __name__ == "__main__":
    uvicorn.run("app.main:app", host=settings.host, port=settings.port, reload=True)
    